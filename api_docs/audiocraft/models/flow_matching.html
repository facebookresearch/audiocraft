<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.5">
<title>audiocraft.models.flow_matching API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>audiocraft.models.flow_matching</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="audiocraft.models.flow_matching.AllCFGTerm"><code class="flex name class">
<span>class <span class="ident">AllCFGTerm</span></span>
<span>(</span><span>conditions, weight)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AllCFGTerm(CFGTerm):
    &#34;&#34;&#34;
    A CFG term that retains all conditions. This class does not drop any condition.
    &#34;&#34;&#34;
    def __init__(self, conditions, weight):
        super().__init__(conditions, weight)
        self.drop_irrelevant_conds()

    def drop_irrelevant_conds(self):
        pass</code></pre>
</details>
<div class="desc"><p>A CFG term that retains all conditions. This class does not drop any condition.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="audiocraft.models.flow_matching.CFGTerm" href="#audiocraft.models.flow_matching.CFGTerm">CFGTerm</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="audiocraft.models.flow_matching.CFGTerm" href="#audiocraft.models.flow_matching.CFGTerm">CFGTerm</a></b></code>:
<ul class="hlist">
<li><code><a title="audiocraft.models.flow_matching.CFGTerm.drop_irrelevant_conds" href="#audiocraft.models.flow_matching.CFGTerm.drop_irrelevant_conds">drop_irrelevant_conds</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="audiocraft.models.flow_matching.CFGTerm"><code class="flex name class">
<span>class <span class="ident">CFGTerm</span></span>
<span>(</span><span>conditions, weight)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CFGTerm:
    &#34;&#34;&#34;
    Base class for Multi Source Classifier-Free Guidance (CFG) terms. This class represents a term in the CFG process,
    which is used to guide the generation process by adjusting the influence of different conditions.
    Attributes:
        conditions (dict): A dictionary of conditions that influence the generation process.
        weight (float): The weight of the CFG term, determining its influence on the generation.
    &#34;&#34;&#34;
    def __init__(self, conditions, weight):
        self.conditions = conditions
        self.weight = weight

    def drop_irrelevant_conds(self, conditions):
        &#34;&#34;&#34;
        Drops irrelevant conditions from the CFG term. This method should be implemented by subclasses.
        Args:
            conditions (dict): The conditions to be filtered.
        Raises:
            NotImplementedError: If the method is not implemented in a subclass.
        &#34;&#34;&#34;
        raise NotImplementedError(&#34;No base implementation for setting generation params.&#34;)</code></pre>
</details>
<div class="desc"><p>Base class for Multi Source Classifier-Free Guidance (CFG) terms. This class represents a term in the CFG process,
which is used to guide the generation process by adjusting the influence of different conditions.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>conditions</code></strong> :&ensp;<code>dict</code></dt>
<dd>A dictionary of conditions that influence the generation process.</dd>
<dt><strong><code>weight</code></strong> :&ensp;<code>float</code></dt>
<dd>The weight of the CFG term, determining its influence on the generation.</dd>
</dl></div>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="audiocraft.models.flow_matching.AllCFGTerm" href="#audiocraft.models.flow_matching.AllCFGTerm">AllCFGTerm</a></li>
<li><a title="audiocraft.models.flow_matching.NullCFGTerm" href="#audiocraft.models.flow_matching.NullCFGTerm">NullCFGTerm</a></li>
<li><a title="audiocraft.models.flow_matching.TextCFGTerm" href="#audiocraft.models.flow_matching.TextCFGTerm">TextCFGTerm</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="audiocraft.models.flow_matching.CFGTerm.drop_irrelevant_conds"><code class="name flex">
<span>def <span class="ident">drop_irrelevant_conds</span></span>(<span>self, conditions)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def drop_irrelevant_conds(self, conditions):
    &#34;&#34;&#34;
    Drops irrelevant conditions from the CFG term. This method should be implemented by subclasses.
    Args:
        conditions (dict): The conditions to be filtered.
    Raises:
        NotImplementedError: If the method is not implemented in a subclass.
    &#34;&#34;&#34;
    raise NotImplementedError(&#34;No base implementation for setting generation params.&#34;)</code></pre>
</details>
<div class="desc"><p>Drops irrelevant conditions from the CFG term. This method should be implemented by subclasses.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>conditions</code></strong> :&ensp;<code>dict</code></dt>
<dd>The conditions to be filtered.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>NotImplementedError</code></dt>
<dd>If the method is not implemented in a subclass.</dd>
</dl></div>
</dd>
</dl>
</dd>
<dt id="audiocraft.models.flow_matching.FMOutput"><code class="flex name class">
<span>class <span class="ident">FMOutput</span></span>
<span>(</span><span>latents: torch.Tensor, mask: torch.Tensor)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@dataclass
class FMOutput:
    latents: torch.Tensor  # [B, T, D]
    mask: torch.Tensor  # [B, T]</code></pre>
</details>
<div class="desc"><p>FMOutput(latents: torch.Tensor, mask: torch.Tensor)</p></div>
<h3>Class variables</h3>
<dl>
<dt id="audiocraft.models.flow_matching.FMOutput.latents"><code class="name">var <span class="ident">latents</span> : torch.Tensor</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="audiocraft.models.flow_matching.FMOutput.mask"><code class="name">var <span class="ident">mask</span> : torch.Tensor</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="audiocraft.models.flow_matching.FlowMatchingModel"><code class="flex name class">
<span>class <span class="ident">FlowMatchingModel</span></span>
<span>(</span><span>condition_provider: <a title="audiocraft.modules.jasco_conditioners.JascoConditioningProvider" href="../modules/jasco_conditioners.html#audiocraft.modules.jasco_conditioners.JascoConditioningProvider">JascoConditioningProvider</a>,<br>fuser: <a title="audiocraft.modules.conditioners.ConditionFuser" href="../modules/conditioners.html#audiocraft.modules.conditioners.ConditionFuser">ConditionFuser</a>,<br>dim: int = 128,<br>num_heads: int = 8,<br>flow_dim: int = 128,<br>chords_dim: int = 0,<br>drums_dim: int = 0,<br>melody_dim: int = 0,<br>hidden_scale: int = 4,<br>norm: str = 'layer_norm',<br>norm_first: bool = False,<br>bias_proj: bool = True,<br>weight_init: str | None = None,<br>depthwise_init: str | None = None,<br>zero_bias_init: bool = False,<br>cfg_dropout: float = 0,<br>cfg_coef: float = 1.0,<br>attribute_dropout: Dict[str, Dict[str, float]] = {},<br>time_embedding_dim: int = 128,<br>**kwargs)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FlowMatchingModel(StreamingModule):
    &#34;&#34;&#34;
    A flow matching model inherits from StreamingModule.
    This model uses a transformer architecture to process and fuse conditions, applying learned embeddings and
    transformations and predicts multi-source guided vector fields.
    Attributes:
        condition_provider (JascoConditioningProvider): Provider for conditioning attributes.
        fuser (ConditionFuser): Fuser for combining multiple conditions.
        dim (int): Dimensionality of the model&#39;s main features.
        num_heads (int): Number of attention heads in the transformer.
        flow_dim (int): Dimensionality of the flow features.
        chords_dim (int): Dimensionality for chord embeddings, if used.
        drums_dim (int): Dimensionality for drums embeddings, if used.
        melody_dim (int): Dimensionality for melody embeddings, if used.
        hidden_scale (int): Scaling factor for the dimensionality of the feedforward network in the transformer.
        norm (str): Type of normalization to use (&#39;layer_norm&#39; or other supported types).
        norm_first (bool): Whether to apply normalization before other operations in the transformer layers.
        bias_proj (bool): Whether to include bias in the projection layers.
        weight_init (Optional[str]): Method for initializing weights.
        depthwise_init (Optional[str]): Method for initializing depthwise convolutional layers.
        zero_bias_init (bool): Whether to initialize biases to zero.
        cfg_dropout (float): Dropout rate for configuration settings.
        cfg_coef (float): Coefficient for configuration influence.
        attribute_dropout (Dict[str, Dict[str, float]]): Dropout rates for specific attributes.
        time_embedding_dim (int): Dimensionality of time embeddings.
        **kwargs: Additional keyword arguments for the transformer.
    Methods:
        __init__: Initializes the model with the specified attributes and configuration.
    &#34;&#34;&#34;
    def __init__(self, condition_provider: JascoConditioningProvider,
                 fuser: ConditionFuser,
                 dim: int = 128,
                 num_heads: int = 8,
                 flow_dim: int = 128,
                 chords_dim: int = 0,
                 drums_dim: int = 0,
                 melody_dim: int = 0,
                 hidden_scale: int = 4,
                 norm: str = &#39;layer_norm&#39;,
                 norm_first: bool = False,
                 bias_proj: bool = True,
                 weight_init: tp.Optional[str] = None,
                 depthwise_init: tp.Optional[str] = None,
                 zero_bias_init: bool = False,
                 cfg_dropout: float = 0,
                 cfg_coef: float = 1.0,
                 attribute_dropout: tp.Dict[str, tp.Dict[str, float]] = {},
                 time_embedding_dim: int = 128,
                 **kwargs):
        super().__init__()
        self.cfg_coef = cfg_coef

        self.cfg_dropout = ClassifierFreeGuidanceDropout(p=cfg_dropout)
        self.att_dropout = AttributeDropout(p=attribute_dropout)
        self.condition_provider = condition_provider
        self.fuser = fuser
        self.dim = dim  # transformer dim
        self.flow_dim = flow_dim
        self.chords_dim = chords_dim
        self.emb = nn.Linear(flow_dim + chords_dim + drums_dim + melody_dim, dim, bias=False)
        if &#39;activation&#39; in kwargs:
            kwargs[&#39;activation&#39;] = get_activation_fn(kwargs[&#39;activation&#39;])

        self.transformer = UnetTransformer(
            d_model=dim, num_heads=num_heads, dim_feedforward=int(hidden_scale * dim),
            norm=norm, norm_first=norm_first,
            layer_class=StreamingTransformerLayer,
            **kwargs)
        self.out_norm: tp.Optional[nn.Module] = None
        if norm_first:
            self.out_norm = create_norm_fn(norm, dim)
        self.linear = nn.Linear(dim, flow_dim, bias=bias_proj)
        self._init_weights(weight_init, depthwise_init, zero_bias_init)
        self._fsdp: tp.Optional[nn.Module]
        self.__dict__[&#39;_fsdp&#39;] = None

        # init time parameter embedding
        self.d_temb1 = time_embedding_dim
        self.d_temb2 = 4 * time_embedding_dim
        self.temb = nn.Module()
        self.temb.dense = nn.ModuleList([
            torch.nn.Linear(self.d_temb1,
                            self.d_temb2),
            torch.nn.Linear(self.d_temb2,
                            self.d_temb2),
        ])
        self.temb_proj = nn.Linear(self.d_temb2, dim)

    def _get_timestep_embedding(self, timesteps, embedding_dim):
        &#34;&#34;&#34;
        #######################################################################################################
        TAKEN FROM: https://github.com/CompVis/stable-diffusion/blob/main/ldm/modules/diffusionmodules/model.py
        #######################################################################################################
        This matches the implementation in Denoising Diffusion Probabilistic Models:
        From Fairseq.
        Build sinusoidal embeddings.
        This matches the implementation in tensor2tensor, but differs slightly
        from the description in Section 3.5 of &#34;Attention Is All You Need&#34;.
        &#34;&#34;&#34;
        assert len(timesteps.shape) == 1

        half_dim = embedding_dim // 2
        emb = math.log(10000) / (half_dim - 1)
        emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)
        emb = emb.to(device=timesteps.device)
        emb = timesteps.float()[:, None] * emb[None, :]
        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)
        if embedding_dim % 2 == 1:  # zero pad
            emb = torch.nn.functional.pad(emb, (0, 1, 0, 0))
        return emb

    def _embed_time_parameter(self, t: torch.Tensor):
        &#34;&#34;&#34;
        #######################################################################################################
        TAKEN FROM: https://github.com/CompVis/stable-diffusion/blob/main/ldm/modules/diffusionmodules/model.py
        #######################################################################################################
        &#34;&#34;&#34;
        temb = self._get_timestep_embedding(t.flatten(), self.d_temb1)
        temb = self.temb.dense[0](temb)
        temb = temb * torch.sigmoid(temb)  # swish activation
        temb = self.temb.dense[1](temb)
        return temb

    def _init_weights(self, weight_init: tp.Optional[str], depthwise_init: tp.Optional[str], zero_bias_init: bool):
        &#34;&#34;&#34;Initialization of the transformer module weights.

        Args:
            weight_init (str, optional): Weight initialization strategy. See ``get_init_fn`` for valid options.
            depthwise_init (str, optional): Depthwise initialization strategy. The following options are valid:
                &#39;current&#39; where the depth corresponds to the current layer index or &#39;global&#39; where the total number
                of layer is used as depth. If not set, no depthwise initialization strategy is used.
            zero_bias_init (bool): Whether to initialize bias to zero or not.
        &#34;&#34;&#34;
        assert depthwise_init is None or depthwise_init in [&#39;current&#39;, &#39;global&#39;]
        assert depthwise_init is None or weight_init is not None, \
            &#34;If &#39;depthwise_init&#39; is defined, a &#39;weight_init&#39; method should be provided.&#34;
        assert not zero_bias_init or weight_init is not None, \
            &#34;If &#39;zero_bias_init&#39;, a &#39;weight_init&#39; method should be provided&#34;

        if weight_init is None:
            return

        init_layer(self.emb, method=weight_init, init_depth=None, zero_bias_init=zero_bias_init)

        for layer_idx, tr_layer in enumerate(self.transformer.layers):
            depth = None
            if depthwise_init == &#39;current&#39;:
                depth = layer_idx + 1
            elif depthwise_init == &#39;global&#39;:
                depth = len(self.transformer.layers)
            init_fn = partial(init_layer, method=weight_init, init_depth=depth, zero_bias_init=zero_bias_init)
            tr_layer.apply(init_fn)

        init_layer(self.linear, method=weight_init, init_depth=None, zero_bias_init=zero_bias_init)

    def _align_seq_length(self,
                          cond: torch.Tensor,
                          seq_len: int = 500):
        # trim if needed
        cond = cond[:, :seq_len, :]

        # pad if needed
        B, T, C = cond.shape
        if T &lt; seq_len:
            cond = torch.cat((cond, torch.zeros((B, seq_len - T, C), dtype=cond.dtype, device=cond.device)), dim=1)

        return cond

    def forward(self,
                latents: torch.Tensor,
                t: torch.Tensor,
                conditions: tp.List[ConditioningAttributes],
                condition_tensors: tp.Optional[ConditionTensors] = None) -&gt; torch.Tensor:
        &#34;&#34;&#34;Apply flow matching forward pass on latents and conditions.
        Given a tensor of noisy latents of shape [B, T, D] with D the flow dim and T the sequence steps,
        and a time parameter tensor t, return the vector field with shape [B, T, D].

        Args:
            latents (torch.Tensor): noisy latents.
            conditions (list of ConditioningAttributes): Conditions to use when modeling
                the given codes. Note that when evaluating multiple time with the same conditioning
                you should pre-compute those and pass them as `condition_tensors`.
            condition_tensors (dict[str, ConditionType], optional): Pre-computed conditioning
                tensors, see `conditions`.
        Returns:
            torch.Tensor: estimated vector field v_theta.
        &#34;&#34;&#34;
        assert condition_tensors is not None, &#34;FlowMatchingModel require pre-calculation of condition tensors&#34;
        assert not conditions, &#34;Shouldn&#39;t pass unprocessed conditions to FlowMatchingModel.&#34;

        B, T, D = latents.shape
        x = latents

        # concat temporal conditions on the feature dimension
        temporal_conds = JascoCondConst.ALL.value
        for cond in temporal_conds:
            if cond not in condition_tensors:
                continue
            c = self._align_seq_length(condition_tensors[cond][0], seq_len=T)
            x = torch.concat((x, c), dim=-1)

        # project to transformer dimension
        input_ = self.emb(x)

        input_, cross_attention_input = self.fuser(input_, condition_tensors)

        # embed time parameter
        t_embs = self._embed_time_parameter(t)

        # add it to cross_attention_input
        cross_attention_input = cross_attention_input + self.temb_proj(t_embs[:, None, :])

        out = self.transformer(input_, cross_attention_src=cross_attention_input)

        if self.out_norm:
            out = self.out_norm(out)
        v_theta = self.linear(out)  # [B, T, D]

        # remove the prefix from the model outputs
        if len(self.fuser.fuse2cond[&#39;prepend&#39;]) &gt; 0:
            v_theta = v_theta[:, :, -T:]

        return v_theta  # [B, T, D]

    def _multi_source_cfg_preprocess(self,
                                     conditions: tp.List[ConditioningAttributes],
                                     cfg_coef_all: float,
                                     cfg_coef_txt: float,
                                     min_weight: float = 1e-6):
        &#34;&#34;&#34;
        Preprocesses the CFG terms for multi-source conditional generation.
        Args:
            conditions (list): A list of conditions to be applied.
            cfg_coef_all (float): The coefficient for all conditions.
            cfg_coef_txt (float): The coefficient for text conditions.
            min_weight (float): The minimal absolute weight for calculating a CFG term.
        Returns:
            tuple: A tuple containing condition_tensors and cfg_terms.
                condition_tensors is a dictionary or ConditionTensors object with tokenized conditions.
                cfg_terms is a list of CFGTerm objects with weights adjusted based on the coefficients.
        &#34;&#34;&#34;
        condition_tensors: tp.Optional[ConditionTensors]
        cfg_terms = []
        if conditions:
            # conditional terms
            cfg_terms = [AllCFGTerm(conditions=conditions, weight=cfg_coef_all),
                         TextCFGTerm(conditions=conditions, weight=cfg_coef_txt,
                                     model_att_dropout=self.att_dropout)]

            # add null term
            cfg_terms.append(NullCFGTerm(conditions=conditions, weight=1 - sum([ct.weight for ct in cfg_terms])))

            # remove terms with negligible weight
            for ct in cfg_terms:
                if abs(ct.weight) &lt; min_weight:
                    cfg_terms.remove(ct)

            conds: tp.List[ConditioningAttributes] = sum([ct.conditions for ct in cfg_terms], [])
            tokenized = self.condition_provider.tokenize(conds)
            condition_tensors = self.condition_provider(tokenized)
        else:
            condition_tensors = {}

        return condition_tensors, cfg_terms

    def estimated_vector_field(self, z, t, condition_tensors=None, cfg_terms=[]):
        &#34;&#34;&#34;
        Estimates the vector field for the given latent variables and time parameter,
        conditioned on the provided conditions.
        Args:
            z (Tensor): The latent variables.
            t (float): The time variable.
            condition_tensors (ConditionTensors, optional): The condition tensors. Defaults to None.
            cfg_terms (list, optional): The list of CFG terms. Defaults to an empty list.
        Returns:
            Tensor: The estimated vector field.
        &#34;&#34;&#34;
        if len(cfg_terms) &gt; 1:
            z = z.repeat(len(cfg_terms), 1, 1)  # duplicate noisy latents for multi-source CFG
        v_thetas = self(latents=z, t=t, conditions=[], condition_tensors=condition_tensors)
        return self._multi_source_cfg_postprocess(v_thetas, cfg_terms)

    def _multi_source_cfg_postprocess(self, v_thetas, cfg_terms):
        &#34;&#34;&#34;
        Postprocesses the vector fields generated for each CFG term to combine them into a single vector field.
        Multi source guidance occurs here.
        Args:
            v_thetas (Tensor): The vector fields for each CFG term.
            cfg_terms (list): The CFG terms used.
        Returns:
            Tensor: The combined vector field.
        &#34;&#34;&#34;
        if len(cfg_terms) &lt;= 1:
            return v_thetas
        v_theta_per_term = v_thetas.chunk(len(cfg_terms))
        return sum([ct.weight * term_vf for ct, term_vf in zip(cfg_terms, v_theta_per_term)])

    @torch.no_grad()
    def generate(self,
                 prompt: tp.Optional[torch.Tensor] = None,
                 conditions: tp.List[ConditioningAttributes] = [],
                 num_samples: tp.Optional[int] = None,
                 max_gen_len: int = 256,
                 callback: tp.Optional[tp.Callable[[int, int], None]] = None,
                 cfg_coef_all: float = 3.0,
                 cfg_coef_txt: float = 1.0,
                 euler: bool = False,
                 euler_steps: int = 100,
                 ode_rtol: float = 1e-5,
                 ode_atol: float = 1e-5,
                 ) -&gt; torch.Tensor:
        &#34;&#34;&#34;
        Generate audio latents given a prompt or unconditionally. This method supports both Euler integration
        and adaptive ODE solving to generate sequences based on the specified conditions and configuration coefficients.

        Args:
            prompt (torch.Tensor, optional): Initial prompt to condition the generation. defaults to None
            conditions (List[ConditioningAttributes]): List of conditioning attributes - text, symbolic or audio.
            num_samples (int, optional): Number of samples to generate.
                                         If None, it is inferred from the number of conditions.
            max_gen_len (int): Maximum length of the generated sequence.
            callback (Callable[[int, int], None], optional): Callback function to monitor the generation process.
            cfg_coef_all (float): Coefficient for the fully conditional CFG term.
            cfg_coef_txt (float): Coefficient for text CFG term.
            euler (bool): If True, use Euler integration, otherwise use adaptive ODE solver.
            euler_steps (int): Number of Euler steps to perform if Euler integration is used.
            ode_rtol (float): ODE solver rtol threshold.
            ode_atol (float): ODE solver atol threshold.

        Returns:
            torch.Tensor: Generated latents, shaped as (num_samples, max_gen_len, feature_dim).
        &#34;&#34;&#34;

        assert not self.training, &#34;generation shouldn&#39;t be used in training mode.&#34;
        first_param = next(iter(self.parameters()))
        device = first_param.device

        # Checking all input shapes are consistent.
        possible_num_samples = []
        if num_samples is not None:
            possible_num_samples.append(num_samples)
        elif prompt is not None:
            possible_num_samples.append(prompt.shape[0])
        elif conditions:
            possible_num_samples.append(len(conditions))
        else:
            possible_num_samples.append(1)
        assert [x == possible_num_samples[0] for x in possible_num_samples], &#34;Inconsistent inputs shapes&#34;
        num_samples = possible_num_samples[0]

        condition_tensors, cfg_terms = self._multi_source_cfg_preprocess(conditions, cfg_coef_all, cfg_coef_txt)

        # flow matching inference
        B, T, D = num_samples, max_gen_len, self.flow_dim

        z_0 = torch.randn((B, T, D), device=device)

        if euler:
            # vanilla Euler intergration
            dt = (1 / euler_steps)
            z = z_0
            t = torch.zeros((1, ), device=device)
            for _ in range(euler_steps):
                v_theta = self.estimated_vector_field(z, t,
                                                      condition_tensors=condition_tensors,
                                                      cfg_terms=cfg_terms)
                z = z + dt * v_theta
                t = t + dt
            z_1 = z
        else:
            # solve with dynamic ode integrator (dopri5)
            t = torch.tensor([0, 1.0 - 1e-5], device=device)
            num_evals = 0

            # define ode vector field function
            def inner_ode_func(t, z):
                nonlocal num_evals
                num_evals += 1
                if callback is not None:
                    ESTIMATED_ODE_SOLVER_STEPS = 300
                    callback(num_evals, ESTIMATED_ODE_SOLVER_STEPS)
                return self.estimated_vector_field(z, t,
                                                   condition_tensors=condition_tensors,
                                                   cfg_terms=cfg_terms)

            ode_opts: dict = {&#34;options&#34;: {}}
            z = odeint(
                inner_ode_func,
                z_0,
                t,
                **{&#34;atol&#34;: ode_atol, &#34;rtol&#34;: ode_rtol, **ode_opts},
            )
            logger.info(&#34;Generated in %d steps&#34;, num_evals)
            z_1 = z[-1]

        return z_1</code></pre>
</details>
<div class="desc"><p>A flow matching model inherits from StreamingModule.
This model uses a transformer architecture to process and fuse conditions, applying learned embeddings and
transformations and predicts multi-source guided vector fields.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>condition_provider</code></strong> :&ensp;<code>JascoConditioningProvider</code></dt>
<dd>Provider for conditioning attributes.</dd>
<dt><strong><code>fuser</code></strong> :&ensp;<code>ConditionFuser</code></dt>
<dd>Fuser for combining multiple conditions.</dd>
<dt><strong><code>dim</code></strong> :&ensp;<code>int</code></dt>
<dd>Dimensionality of the model's main features.</dd>
<dt><strong><code>num_heads</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of attention heads in the transformer.</dd>
<dt><strong><code>flow_dim</code></strong> :&ensp;<code>int</code></dt>
<dd>Dimensionality of the flow features.</dd>
<dt><strong><code>chords_dim</code></strong> :&ensp;<code>int</code></dt>
<dd>Dimensionality for chord embeddings, if used.</dd>
<dt><strong><code>drums_dim</code></strong> :&ensp;<code>int</code></dt>
<dd>Dimensionality for drums embeddings, if used.</dd>
<dt><strong><code>melody_dim</code></strong> :&ensp;<code>int</code></dt>
<dd>Dimensionality for melody embeddings, if used.</dd>
<dt><strong><code>hidden_scale</code></strong> :&ensp;<code>int</code></dt>
<dd>Scaling factor for the dimensionality of the feedforward network in the transformer.</dd>
<dt><strong><code>norm</code></strong> :&ensp;<code>str</code></dt>
<dd>Type of normalization to use ('layer_norm' or other supported types).</dd>
<dt><strong><code>norm_first</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to apply normalization before other operations in the transformer layers.</dd>
<dt><strong><code>bias_proj</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to include bias in the projection layers.</dd>
<dt><strong><code>weight_init</code></strong> :&ensp;<code>Optional[str]</code></dt>
<dd>Method for initializing weights.</dd>
<dt><strong><code>depthwise_init</code></strong> :&ensp;<code>Optional[str]</code></dt>
<dd>Method for initializing depthwise convolutional layers.</dd>
<dt><strong><code>zero_bias_init</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to initialize biases to zero.</dd>
<dt><strong><code>cfg_dropout</code></strong> :&ensp;<code>float</code></dt>
<dd>Dropout rate for configuration settings.</dd>
<dt><strong><code>cfg_coef</code></strong> :&ensp;<code>float</code></dt>
<dd>Coefficient for configuration influence.</dd>
<dt><strong><code>attribute_dropout</code></strong> :&ensp;<code>Dict[str, Dict[str, float]]</code></dt>
<dd>Dropout rates for specific attributes.</dd>
<dt><strong><code>time_embedding_dim</code></strong> :&ensp;<code>int</code></dt>
<dd>Dimensionality of time embeddings.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Additional keyword arguments for the transformer.</dd>
</dl>
<h2 id="methods">Methods</h2>
<p><strong>init</strong>: Initializes the model with the specified attributes and configuration.</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="audiocraft.modules.streaming.StreamingModule" href="../modules/streaming.html#audiocraft.modules.streaming.StreamingModule">StreamingModule</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="audiocraft.models.flow_matching.FlowMatchingModel.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="audiocraft.models.flow_matching.FlowMatchingModel.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="audiocraft.models.flow_matching.FlowMatchingModel.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="audiocraft.models.flow_matching.FlowMatchingModel.estimated_vector_field"><code class="name flex">
<span>def <span class="ident">estimated_vector_field</span></span>(<span>self, z, t, condition_tensors=None, cfg_terms=[])</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def estimated_vector_field(self, z, t, condition_tensors=None, cfg_terms=[]):
    &#34;&#34;&#34;
    Estimates the vector field for the given latent variables and time parameter,
    conditioned on the provided conditions.
    Args:
        z (Tensor): The latent variables.
        t (float): The time variable.
        condition_tensors (ConditionTensors, optional): The condition tensors. Defaults to None.
        cfg_terms (list, optional): The list of CFG terms. Defaults to an empty list.
    Returns:
        Tensor: The estimated vector field.
    &#34;&#34;&#34;
    if len(cfg_terms) &gt; 1:
        z = z.repeat(len(cfg_terms), 1, 1)  # duplicate noisy latents for multi-source CFG
    v_thetas = self(latents=z, t=t, conditions=[], condition_tensors=condition_tensors)
    return self._multi_source_cfg_postprocess(v_thetas, cfg_terms)</code></pre>
</details>
<div class="desc"><p>Estimates the vector field for the given latent variables and time parameter,
conditioned on the provided conditions.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>z</code></strong> :&ensp;<code>Tensor</code></dt>
<dd>The latent variables.</dd>
<dt><strong><code>t</code></strong> :&ensp;<code>float</code></dt>
<dd>The time variable.</dd>
<dt><strong><code>condition_tensors</code></strong> :&ensp;<code>ConditionTensors</code>, optional</dt>
<dd>The condition tensors. Defaults to None.</dd>
<dt><strong><code>cfg_terms</code></strong> :&ensp;<code>list</code>, optional</dt>
<dd>The list of CFG terms. Defaults to an empty list.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Tensor</code></dt>
<dd>The estimated vector field.</dd>
</dl></div>
</dd>
<dt id="audiocraft.models.flow_matching.FlowMatchingModel.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self,<br>latents: torch.Tensor,<br>t: torch.Tensor,<br>conditions: List[<a title="audiocraft.modules.conditioners.ConditioningAttributes" href="../modules/conditioners.html#audiocraft.modules.conditioners.ConditioningAttributes">ConditioningAttributes</a>],<br>condition_tensors: Dict[str, Tuple[torch.Tensor, torch.Tensor]] | None = None) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self,
            latents: torch.Tensor,
            t: torch.Tensor,
            conditions: tp.List[ConditioningAttributes],
            condition_tensors: tp.Optional[ConditionTensors] = None) -&gt; torch.Tensor:
    &#34;&#34;&#34;Apply flow matching forward pass on latents and conditions.
    Given a tensor of noisy latents of shape [B, T, D] with D the flow dim and T the sequence steps,
    and a time parameter tensor t, return the vector field with shape [B, T, D].

    Args:
        latents (torch.Tensor): noisy latents.
        conditions (list of ConditioningAttributes): Conditions to use when modeling
            the given codes. Note that when evaluating multiple time with the same conditioning
            you should pre-compute those and pass them as `condition_tensors`.
        condition_tensors (dict[str, ConditionType], optional): Pre-computed conditioning
            tensors, see `conditions`.
    Returns:
        torch.Tensor: estimated vector field v_theta.
    &#34;&#34;&#34;
    assert condition_tensors is not None, &#34;FlowMatchingModel require pre-calculation of condition tensors&#34;
    assert not conditions, &#34;Shouldn&#39;t pass unprocessed conditions to FlowMatchingModel.&#34;

    B, T, D = latents.shape
    x = latents

    # concat temporal conditions on the feature dimension
    temporal_conds = JascoCondConst.ALL.value
    for cond in temporal_conds:
        if cond not in condition_tensors:
            continue
        c = self._align_seq_length(condition_tensors[cond][0], seq_len=T)
        x = torch.concat((x, c), dim=-1)

    # project to transformer dimension
    input_ = self.emb(x)

    input_, cross_attention_input = self.fuser(input_, condition_tensors)

    # embed time parameter
    t_embs = self._embed_time_parameter(t)

    # add it to cross_attention_input
    cross_attention_input = cross_attention_input + self.temb_proj(t_embs[:, None, :])

    out = self.transformer(input_, cross_attention_src=cross_attention_input)

    if self.out_norm:
        out = self.out_norm(out)
    v_theta = self.linear(out)  # [B, T, D]

    # remove the prefix from the model outputs
    if len(self.fuser.fuse2cond[&#39;prepend&#39;]) &gt; 0:
        v_theta = v_theta[:, :, -T:]

    return v_theta  # [B, T, D]</code></pre>
</details>
<div class="desc"><p>Apply flow matching forward pass on latents and conditions.
Given a tensor of noisy latents of shape [B, T, D] with D the flow dim and T the sequence steps,
and a time parameter tensor t, return the vector field with shape [B, T, D].</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>latents</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>noisy latents.</dd>
<dt><strong><code>conditions</code></strong> :&ensp;<code>list</code> of <code>ConditioningAttributes</code></dt>
<dd>Conditions to use when modeling
the given codes. Note that when evaluating multiple time with the same conditioning
you should pre-compute those and pass them as <code>condition_tensors</code>.</dd>
<dt><strong><code>condition_tensors</code></strong> :&ensp;<code>dict[str, ConditionType]</code>, optional</dt>
<dd>Pre-computed conditioning
tensors, see <code>conditions</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torch.Tensor</code></dt>
<dd>estimated vector field v_theta.</dd>
</dl></div>
</dd>
<dt id="audiocraft.models.flow_matching.FlowMatchingModel.generate"><code class="name flex">
<span>def <span class="ident">generate</span></span>(<span>self,<br>prompt: torch.Tensor | None = None,<br>conditions: List[<a title="audiocraft.modules.conditioners.ConditioningAttributes" href="../modules/conditioners.html#audiocraft.modules.conditioners.ConditioningAttributes">ConditioningAttributes</a>] = [],<br>num_samples: int | None = None,<br>max_gen_len: int = 256,<br>callback: Callable[[int, int], None] | None = None,<br>cfg_coef_all: float = 3.0,<br>cfg_coef_txt: float = 1.0,<br>euler: bool = False,<br>euler_steps: int = 100,<br>ode_rtol: float = 1e-05,<br>ode_atol: float = 1e-05) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@torch.no_grad()
def generate(self,
             prompt: tp.Optional[torch.Tensor] = None,
             conditions: tp.List[ConditioningAttributes] = [],
             num_samples: tp.Optional[int] = None,
             max_gen_len: int = 256,
             callback: tp.Optional[tp.Callable[[int, int], None]] = None,
             cfg_coef_all: float = 3.0,
             cfg_coef_txt: float = 1.0,
             euler: bool = False,
             euler_steps: int = 100,
             ode_rtol: float = 1e-5,
             ode_atol: float = 1e-5,
             ) -&gt; torch.Tensor:
    &#34;&#34;&#34;
    Generate audio latents given a prompt or unconditionally. This method supports both Euler integration
    and adaptive ODE solving to generate sequences based on the specified conditions and configuration coefficients.

    Args:
        prompt (torch.Tensor, optional): Initial prompt to condition the generation. defaults to None
        conditions (List[ConditioningAttributes]): List of conditioning attributes - text, symbolic or audio.
        num_samples (int, optional): Number of samples to generate.
                                     If None, it is inferred from the number of conditions.
        max_gen_len (int): Maximum length of the generated sequence.
        callback (Callable[[int, int], None], optional): Callback function to monitor the generation process.
        cfg_coef_all (float): Coefficient for the fully conditional CFG term.
        cfg_coef_txt (float): Coefficient for text CFG term.
        euler (bool): If True, use Euler integration, otherwise use adaptive ODE solver.
        euler_steps (int): Number of Euler steps to perform if Euler integration is used.
        ode_rtol (float): ODE solver rtol threshold.
        ode_atol (float): ODE solver atol threshold.

    Returns:
        torch.Tensor: Generated latents, shaped as (num_samples, max_gen_len, feature_dim).
    &#34;&#34;&#34;

    assert not self.training, &#34;generation shouldn&#39;t be used in training mode.&#34;
    first_param = next(iter(self.parameters()))
    device = first_param.device

    # Checking all input shapes are consistent.
    possible_num_samples = []
    if num_samples is not None:
        possible_num_samples.append(num_samples)
    elif prompt is not None:
        possible_num_samples.append(prompt.shape[0])
    elif conditions:
        possible_num_samples.append(len(conditions))
    else:
        possible_num_samples.append(1)
    assert [x == possible_num_samples[0] for x in possible_num_samples], &#34;Inconsistent inputs shapes&#34;
    num_samples = possible_num_samples[0]

    condition_tensors, cfg_terms = self._multi_source_cfg_preprocess(conditions, cfg_coef_all, cfg_coef_txt)

    # flow matching inference
    B, T, D = num_samples, max_gen_len, self.flow_dim

    z_0 = torch.randn((B, T, D), device=device)

    if euler:
        # vanilla Euler intergration
        dt = (1 / euler_steps)
        z = z_0
        t = torch.zeros((1, ), device=device)
        for _ in range(euler_steps):
            v_theta = self.estimated_vector_field(z, t,
                                                  condition_tensors=condition_tensors,
                                                  cfg_terms=cfg_terms)
            z = z + dt * v_theta
            t = t + dt
        z_1 = z
    else:
        # solve with dynamic ode integrator (dopri5)
        t = torch.tensor([0, 1.0 - 1e-5], device=device)
        num_evals = 0

        # define ode vector field function
        def inner_ode_func(t, z):
            nonlocal num_evals
            num_evals += 1
            if callback is not None:
                ESTIMATED_ODE_SOLVER_STEPS = 300
                callback(num_evals, ESTIMATED_ODE_SOLVER_STEPS)
            return self.estimated_vector_field(z, t,
                                               condition_tensors=condition_tensors,
                                               cfg_terms=cfg_terms)

        ode_opts: dict = {&#34;options&#34;: {}}
        z = odeint(
            inner_ode_func,
            z_0,
            t,
            **{&#34;atol&#34;: ode_atol, &#34;rtol&#34;: ode_rtol, **ode_opts},
        )
        logger.info(&#34;Generated in %d steps&#34;, num_evals)
        z_1 = z[-1]

    return z_1</code></pre>
</details>
<div class="desc"><p>Generate audio latents given a prompt or unconditionally. This method supports both Euler integration
and adaptive ODE solving to generate sequences based on the specified conditions and configuration coefficients.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>prompt</code></strong> :&ensp;<code>torch.Tensor</code>, optional</dt>
<dd>Initial prompt to condition the generation. defaults to None</dd>
<dt><strong><code>conditions</code></strong> :&ensp;<code>List[ConditioningAttributes]</code></dt>
<dd>List of conditioning attributes - text, symbolic or audio.</dd>
<dt><strong><code>num_samples</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of samples to generate.
If None, it is inferred from the number of conditions.</dd>
<dt><strong><code>max_gen_len</code></strong> :&ensp;<code>int</code></dt>
<dd>Maximum length of the generated sequence.</dd>
<dt><strong><code>callback</code></strong> :&ensp;<code>Callable[[int, int], None]</code>, optional</dt>
<dd>Callback function to monitor the generation process.</dd>
<dt><strong><code>cfg_coef_all</code></strong> :&ensp;<code>float</code></dt>
<dd>Coefficient for the fully conditional CFG term.</dd>
<dt><strong><code>cfg_coef_txt</code></strong> :&ensp;<code>float</code></dt>
<dd>Coefficient for text CFG term.</dd>
<dt><strong><code>euler</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, use Euler integration, otherwise use adaptive ODE solver.</dd>
<dt><strong><code>euler_steps</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of Euler steps to perform if Euler integration is used.</dd>
<dt><strong><code>ode_rtol</code></strong> :&ensp;<code>float</code></dt>
<dd>ODE solver rtol threshold.</dd>
<dt><strong><code>ode_atol</code></strong> :&ensp;<code>float</code></dt>
<dd>ODE solver atol threshold.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torch.Tensor</code></dt>
<dd>Generated latents, shaped as (num_samples, max_gen_len, feature_dim).</dd>
</dl></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="audiocraft.modules.streaming.StreamingModule" href="../modules/streaming.html#audiocraft.modules.streaming.StreamingModule">StreamingModule</a></b></code>:
<ul class="hlist">
<li><code><a title="audiocraft.modules.streaming.StreamingModule.flush" href="../modules/streaming.html#audiocraft.modules.streaming.StreamingModule.flush">flush</a></code></li>
<li><code><a title="audiocraft.modules.streaming.StreamingModule.get_streaming_state" href="../modules/streaming.html#audiocraft.modules.streaming.StreamingModule.get_streaming_state">get_streaming_state</a></code></li>
<li><code><a title="audiocraft.modules.streaming.StreamingModule.reset_streaming" href="../modules/streaming.html#audiocraft.modules.streaming.StreamingModule.reset_streaming">reset_streaming</a></code></li>
<li><code><a title="audiocraft.modules.streaming.StreamingModule.set_streaming_state" href="../modules/streaming.html#audiocraft.modules.streaming.StreamingModule.set_streaming_state">set_streaming_state</a></code></li>
<li><code><a title="audiocraft.modules.streaming.StreamingModule.streaming" href="../modules/streaming.html#audiocraft.modules.streaming.StreamingModule.streaming">streaming</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="audiocraft.models.flow_matching.NullCFGTerm"><code class="flex name class">
<span>class <span class="ident">NullCFGTerm</span></span>
<span>(</span><span>conditions, weight)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class NullCFGTerm(CFGTerm):
    &#34;&#34;&#34;
    A CFG term that drops all conditions, effectively nullifying their influence.
    &#34;&#34;&#34;
    def __init__(self, conditions, weight):
        super().__init__(conditions, weight)
        self.drop_irrelevant_conds()

    def drop_irrelevant_conds(self):
        &#34;&#34;&#34;
        Drops all conditions by applying a dropout with probability 1.0, effectively nullifying their influence.
        &#34;&#34;&#34;
        self.conditions = ClassifierFreeGuidanceDropout(p=1.0)(
                                                        samples=self.conditions,
                                                        cond_types=[&#34;wav&#34;, &#34;text&#34;, &#34;symbolic&#34;])</code></pre>
</details>
<div class="desc"><p>A CFG term that drops all conditions, effectively nullifying their influence.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="audiocraft.models.flow_matching.CFGTerm" href="#audiocraft.models.flow_matching.CFGTerm">CFGTerm</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="audiocraft.models.flow_matching.NullCFGTerm.drop_irrelevant_conds"><code class="name flex">
<span>def <span class="ident">drop_irrelevant_conds</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def drop_irrelevant_conds(self):
    &#34;&#34;&#34;
    Drops all conditions by applying a dropout with probability 1.0, effectively nullifying their influence.
    &#34;&#34;&#34;
    self.conditions = ClassifierFreeGuidanceDropout(p=1.0)(
                                                    samples=self.conditions,
                                                    cond_types=[&#34;wav&#34;, &#34;text&#34;, &#34;symbolic&#34;])</code></pre>
</details>
<div class="desc"><p>Drops all conditions by applying a dropout with probability 1.0, effectively nullifying their influence.</p></div>
</dd>
</dl>
</dd>
<dt id="audiocraft.models.flow_matching.TextCFGTerm"><code class="flex name class">
<span>class <span class="ident">TextCFGTerm</span></span>
<span>(</span><span>conditions, weight, model_att_dropout)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TextCFGTerm(CFGTerm):
    &#34;&#34;&#34;
    A CFG term that selectively drops conditions based on specified dropout probabilities for different types
    of conditions, such as &#39;symbolic&#39; and &#39;wav&#39;.
    &#34;&#34;&#34;
    def __init__(self, conditions, weight, model_att_dropout):
        &#34;&#34;&#34;
        Initializes a TextCFGTerm with specified conditions, weight, and model attention dropout configuration.
        Args:
            conditions (dict): The conditions to be used in the CFG process.
            weight (float): The weight of the CFG term.
            model_att_dropout (object): The attribute dropouts used by the model.
        &#34;&#34;&#34;
        super().__init__(conditions, weight)
        if &#39;symbolic&#39; in model_att_dropout.p:
            self.drop_symbolics = {k: 1.0 for k in model_att_dropout.p[&#39;symbolic&#39;].keys()}
        else:
            self.drop_symbolics = {}
        if &#39;wav&#39; in model_att_dropout.p:
            self.drop_wav = {k: 1.0 for k in model_att_dropout.p[&#39;wav&#39;].keys()}
        else:
            self.drop_wav = {}
        self.drop_irrelevant_conds()

    def drop_irrelevant_conds(self):
        self.conditions = AttributeDropout({&#39;symbolic&#39;: self.drop_symbolics,
                                            &#39;wav&#39;: self.drop_wav})(self.conditions)  # drop temporal conds</code></pre>
</details>
<div class="desc"><p>A CFG term that selectively drops conditions based on specified dropout probabilities for different types
of conditions, such as 'symbolic' and 'wav'.</p>
<p>Initializes a TextCFGTerm with specified conditions, weight, and model attention dropout configuration.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>conditions</code></strong> :&ensp;<code>dict</code></dt>
<dd>The conditions to be used in the CFG process.</dd>
<dt><strong><code>weight</code></strong> :&ensp;<code>float</code></dt>
<dd>The weight of the CFG term.</dd>
<dt><strong><code>model_att_dropout</code></strong> :&ensp;<code>object</code></dt>
<dd>The attribute dropouts used by the model.</dd>
</dl></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="audiocraft.models.flow_matching.CFGTerm" href="#audiocraft.models.flow_matching.CFGTerm">CFGTerm</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="audiocraft.models.flow_matching.CFGTerm" href="#audiocraft.models.flow_matching.CFGTerm">CFGTerm</a></b></code>:
<ul class="hlist">
<li><code><a title="audiocraft.models.flow_matching.CFGTerm.drop_irrelevant_conds" href="#audiocraft.models.flow_matching.CFGTerm.drop_irrelevant_conds">drop_irrelevant_conds</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="audiocraft.models" href="index.html">audiocraft.models</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="audiocraft.models.flow_matching.AllCFGTerm" href="#audiocraft.models.flow_matching.AllCFGTerm">AllCFGTerm</a></code></h4>
</li>
<li>
<h4><code><a title="audiocraft.models.flow_matching.CFGTerm" href="#audiocraft.models.flow_matching.CFGTerm">CFGTerm</a></code></h4>
<ul class="">
<li><code><a title="audiocraft.models.flow_matching.CFGTerm.drop_irrelevant_conds" href="#audiocraft.models.flow_matching.CFGTerm.drop_irrelevant_conds">drop_irrelevant_conds</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="audiocraft.models.flow_matching.FMOutput" href="#audiocraft.models.flow_matching.FMOutput">FMOutput</a></code></h4>
<ul class="">
<li><code><a title="audiocraft.models.flow_matching.FMOutput.latents" href="#audiocraft.models.flow_matching.FMOutput.latents">latents</a></code></li>
<li><code><a title="audiocraft.models.flow_matching.FMOutput.mask" href="#audiocraft.models.flow_matching.FMOutput.mask">mask</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="audiocraft.models.flow_matching.FlowMatchingModel" href="#audiocraft.models.flow_matching.FlowMatchingModel">FlowMatchingModel</a></code></h4>
<ul class="">
<li><code><a title="audiocraft.models.flow_matching.FlowMatchingModel.call_super_init" href="#audiocraft.models.flow_matching.FlowMatchingModel.call_super_init">call_super_init</a></code></li>
<li><code><a title="audiocraft.models.flow_matching.FlowMatchingModel.dump_patches" href="#audiocraft.models.flow_matching.FlowMatchingModel.dump_patches">dump_patches</a></code></li>
<li><code><a title="audiocraft.models.flow_matching.FlowMatchingModel.estimated_vector_field" href="#audiocraft.models.flow_matching.FlowMatchingModel.estimated_vector_field">estimated_vector_field</a></code></li>
<li><code><a title="audiocraft.models.flow_matching.FlowMatchingModel.forward" href="#audiocraft.models.flow_matching.FlowMatchingModel.forward">forward</a></code></li>
<li><code><a title="audiocraft.models.flow_matching.FlowMatchingModel.generate" href="#audiocraft.models.flow_matching.FlowMatchingModel.generate">generate</a></code></li>
<li><code><a title="audiocraft.models.flow_matching.FlowMatchingModel.training" href="#audiocraft.models.flow_matching.FlowMatchingModel.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="audiocraft.models.flow_matching.NullCFGTerm" href="#audiocraft.models.flow_matching.NullCFGTerm">NullCFGTerm</a></code></h4>
<ul class="">
<li><code><a title="audiocraft.models.flow_matching.NullCFGTerm.drop_irrelevant_conds" href="#audiocraft.models.flow_matching.NullCFGTerm.drop_irrelevant_conds">drop_irrelevant_conds</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="audiocraft.models.flow_matching.TextCFGTerm" href="#audiocraft.models.flow_matching.TextCFGTerm">TextCFGTerm</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.5</a>.</p>
</footer>
</body>
</html>
