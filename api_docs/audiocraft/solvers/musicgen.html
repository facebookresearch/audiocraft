<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.5">
<title>audiocraft.solvers.musicgen API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>audiocraft.solvers.musicgen</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="audiocraft.solvers.musicgen.MusicGenSolver"><code class="flex name class">
<span>class <span class="ident">MusicGenSolver</span></span>
<span>(</span><span>cfg: omegaconf.dictconfig.DictConfig)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MusicGenSolver(base.StandardSolver):
    &#34;&#34;&#34;Solver for MusicGen training task.

    Used in: https://arxiv.org/abs/2306.05284
    &#34;&#34;&#34;
    DATASET_TYPE: builders.DatasetType = builders.DatasetType.MUSIC

    def __init__(self, cfg: omegaconf.DictConfig):
        super().__init__(cfg)
        # easier access to sampling parameters
        self.generation_params = {
            &#39;use_sampling&#39;: self.cfg.generate.lm.use_sampling,
            &#39;temp&#39;: self.cfg.generate.lm.temp,
            &#39;top_k&#39;: self.cfg.generate.lm.top_k,
            &#39;top_p&#39;: self.cfg.generate.lm.top_p,
        }
        self._best_metric_name: tp.Optional[str] = &#39;ce&#39;

        self._cached_batch_writer = None
        self._cached_batch_loader = None
        if cfg.cache.path:
            if cfg.cache.write:
                self._cached_batch_writer = CachedBatchWriter(Path(cfg.cache.path))
                if self.cfg.cache.write_num_shards:
                    self.logger.warning(&#34;Multiple shard cache, best_metric_name will be set to None.&#34;)
                    self._best_metric_name = None
            else:
                self._cached_batch_loader = CachedBatchLoader(
                    Path(cfg.cache.path), cfg.dataset.batch_size, cfg.dataset.num_workers,
                    min_length=self.cfg.optim.updates_per_epoch or 1)
                self.dataloaders[&#39;original_train&#39;] = self.dataloaders[&#39;train&#39;]
                self.dataloaders[&#39;train&#39;] = self._cached_batch_loader  # type: ignore

    @staticmethod
    def get_eval_solver_from_sig(sig: str, dtype: tp.Optional[str] = None,
                                 device: tp.Optional[str] = None, autocast: bool = True,
                                 batch_size: tp.Optional[int] = None,
                                 override_cfg: tp.Optional[tp.Union[dict, omegaconf.DictConfig]] = None,
                                 **kwargs):
        &#34;&#34;&#34;Mostly a convenience function around magma.train.get_solver_from_sig,
        populating all the proper param, deactivating EMA, FSDP, loading the best state,
        basically all you need to get a solver ready to &#34;play&#34; with in single GPU mode
        and with minimal memory overhead.

        Args:
            sig (str): signature to load.
            dtype (str or None): potential dtype, as a string, i.e. &#39;float16&#39;.
            device (str or None): potential device, as a string, i.e. &#39;cuda&#39;.
            override_cfg (dict or omegaconf.DictConfig or None): potential device, as a string, i.e. &#39;cuda&#39;.
        &#34;&#34;&#34;
        from audiocraft import train
        our_override_cfg: tp.Dict[str, tp.Any] = {&#39;optim&#39;: {&#39;ema&#39;: {&#39;use&#39;: False}}}
        our_override_cfg[&#39;autocast&#39;] = autocast
        if dtype is not None:
            our_override_cfg[&#39;dtype&#39;] = dtype
        if device is not None:
            our_override_cfg[&#39;device&#39;] = device
        if batch_size is not None:
            our_override_cfg[&#39;dataset&#39;] = {&#39;batch_size&#39;: batch_size}
        if override_cfg is None:
            override_cfg = {}
        override_cfg = omegaconf.OmegaConf.merge(
            omegaconf.DictConfig(override_cfg), omegaconf.DictConfig(our_override_cfg))  # type: ignore
        solver = train.get_solver_from_sig(
            sig, override_cfg=override_cfg,
            load_best=True, disable_fsdp=True,
            ignore_state_keys=[&#39;optimizer&#39;, &#39;ema&#39;], **kwargs)
        solver.model.eval()
        return solver

    def get_formatter(self, stage_name: str) -&gt; flashy.Formatter:
        return flashy.Formatter({
            &#39;lr&#39;: &#39;.2E&#39;,
            &#39;ce&#39;: &#39;.3f&#39;,
            &#39;ppl&#39;: &#39;.3f&#39;,
            &#39;grad_norm&#39;: &#39;.3E&#39;,
        }, exclude_keys=[&#39;ce_q*&#39;, &#39;ppl_q*&#39;])

    @property
    def best_metric_name(self) -&gt; tp.Optional[str]:
        return self._best_metric_name

    def initialize_optimization(self) -&gt; None:
        if self.cfg.fsdp.use:
            assert not self.cfg.autocast, &#34;Cannot use autocast with fsdp&#34;
            self.model = self.wrap_with_fsdp(self.model)
        self.register_ema(&#39;model&#39;)
        # initialize optimization
        self.optimizer = builders.get_optimizer(builders.get_optim_parameter_groups(self.model), self.cfg.optim)
        self.lr_scheduler = builders.get_lr_scheduler(self.optimizer, self.cfg.schedule, self.total_updates)
        self.register_stateful(&#39;model&#39;, &#39;optimizer&#39;, &#39;lr_scheduler&#39;)
        self.register_best_state(&#39;model&#39;)
        self.autocast_dtype = {
            &#39;float16&#39;: torch.float16, &#39;bfloat16&#39;: torch.bfloat16
        }[self.cfg.autocast_dtype]
        self.scaler: tp.Optional[torch.cuda.amp.GradScaler] = None
        if self.cfg.fsdp.use:
            need_scaler = self.cfg.fsdp.param_dtype == &#39;float16&#39;
        else:
            need_scaler = self.cfg.autocast and self.autocast_dtype is torch.float16
        if need_scaler:
            if self.cfg.fsdp.use:
                from torch.distributed.fsdp.sharded_grad_scaler import ShardedGradScaler
                self.scaler = ShardedGradScaler()  # type: ignore
            else:
                self.scaler = torch.cuda.amp.GradScaler()
            self.register_stateful(&#39;scaler&#39;)

    def build_model(self) -&gt; None:
        &#34;&#34;&#34;Instantiate models and optimizer.&#34;&#34;&#34;
        # we can potentially not use all quantizers with which the EnCodec model was trained
        # (e.g. we trained the model with quantizers dropout)
        self.compression_model = CompressionSolver.wrapped_model_from_checkpoint(
            self.cfg, self.cfg.compression_model_checkpoint, device=self.device)
        assert self.compression_model.sample_rate == self.cfg.sample_rate, (
            f&#34;Compression model sample rate is {self.compression_model.sample_rate} but &#34;
            f&#34;Solver sample rate is {self.cfg.sample_rate}.&#34;
            )
        # ensure we have matching configuration between LM and compression model
        assert self.cfg.transformer_lm.card == self.compression_model.cardinality, (
            &#34;Cardinalities of the LM and compression model don&#39;t match: &#34;,
            f&#34;LM cardinality is {self.cfg.transformer_lm.card} vs &#34;,
            f&#34;compression model cardinality is {self.compression_model.cardinality}&#34;
        )
        assert self.cfg.transformer_lm.n_q == self.compression_model.num_codebooks, (
            &#34;Numbers of codebooks of the LM and compression models don&#39;t match: &#34;,
            f&#34;LM number of codebooks is {self.cfg.transformer_lm.n_q} vs &#34;,
            f&#34;compression model numer of codebooks is {self.compression_model.num_codebooks}&#34;
        )
        self.logger.info(&#34;Compression model has %d codebooks with %d cardinality, and a framerate of %d&#34;,
                         self.compression_model.num_codebooks, self.compression_model.cardinality,
                         self.compression_model.frame_rate)
        # instantiate LM model
        self.model: tp.Union[models.LMModel, models.FlowMatchingModel] = models.builders.get_lm_model(
            self.cfg).to(self.device)

        # initialize optimization
        self.initialize_optimization()

    def build_dataloaders(self) -&gt; None:
        &#34;&#34;&#34;Instantiate audio dataloaders for each stage.&#34;&#34;&#34;
        self.dataloaders = builders.get_audio_datasets(self.cfg, dataset_type=self.DATASET_TYPE)

    def show(self) -&gt; None:
        &#34;&#34;&#34;Show the compression model and LM model.&#34;&#34;&#34;
        self.logger.info(&#34;Compression model:&#34;)
        self.log_model_summary(self.compression_model)
        self.logger.info(&#34;LM model:&#34;)
        self.log_model_summary(self.model)

    def load_state_dict(self, state: dict) -&gt; None:
        if &#39;condition_provider&#39; in state:
            model_state = state[&#39;model&#39;]
            condition_provider_state = state.pop(&#39;condition_provider&#39;)
            prefix = &#39;condition_provider.&#39;
            for key, value in condition_provider_state.items():
                key = prefix + key
                assert key not in model_state
                model_state[key] = value
        if &#39;compression_model&#39; in state:
            # We used to store the `compression_model` state in the checkpoint, however
            # this is in general not needed, as the compression model should always be readable
            # from the original `cfg.compression_model_checkpoint` location.
            compression_model_state = state.pop(&#39;compression_model&#39;)
            before_hash = model_hash(self.compression_model)
            self.compression_model.load_state_dict(compression_model_state)
            after_hash = model_hash(self.compression_model)
            if before_hash != after_hash:
                raise RuntimeError(
                    &#34;The compression model state inside the checkpoint is different&#34;
                    &#34; from the one obtained from compression_model_checkpoint...&#34;
                    &#34;We do not support altering the compression model inside the LM &#34;
                    &#34;checkpoint as parts of the code, in particular for running eval post-training &#34;
                    &#34;will use the compression_model_checkpoint as the source of truth.&#34;)

        super().load_state_dict(state)

    def load_from_pretrained(self, name: str):
        # TODO: support native HF versions of MusicGen.
        lm_pkg = models.loaders.load_lm_model_ckpt(name)
        state: dict = {
            &#39;best_state&#39;: {
                &#39;model&#39;: lm_pkg[&#39;best_state&#39;],
            },
        }
        return state

    def _compute_cross_entropy(
        self, logits: torch.Tensor, targets: torch.Tensor, mask: torch.Tensor
    ) -&gt; tp.Tuple[torch.Tensor, tp.List[torch.Tensor]]:
        &#34;&#34;&#34;Compute cross entropy between multi-codebook targets and model&#39;s logits.
        The cross entropy is computed per codebook to provide codebook-level cross entropy.
        Valid timesteps for each of the codebook are pulled from the mask, where invalid
        timesteps are set to 0.

        Args:
            logits (torch.Tensor): Model&#39;s logits of shape [B, K, T, card].
            targets (torch.Tensor): Target codes, of shape [B, K, T].
            mask (torch.Tensor): Mask for valid target codes, of shape [B, K, T].
        Returns:
            ce (torch.Tensor): Cross entropy averaged over the codebooks
            ce_per_codebook (list of torch.Tensor): Cross entropy per codebook (detached).
        &#34;&#34;&#34;
        B, K, T = targets.shape
        assert logits.shape[:-1] == targets.shape
        assert mask.shape == targets.shape
        ce = torch.zeros([], device=targets.device)
        ce_per_codebook: tp.List[torch.Tensor] = []
        for k in range(K):
            logits_k = logits[:, k, ...].contiguous().view(-1, logits.size(-1))  # [B x T, card]
            targets_k = targets[:, k, ...].contiguous().view(-1)  # [B x T]
            mask_k = mask[:, k, ...].contiguous().view(-1)  # [B x T]
            ce_targets = targets_k[mask_k]
            ce_logits = logits_k[mask_k]
            q_ce = F.cross_entropy(ce_logits, ce_targets)
            ce += q_ce
            ce_per_codebook.append(q_ce.detach())
        # average cross entropy across codebooks
        ce = ce / K
        return ce, ce_per_codebook

    def _get_audio_tokens(self, audio: torch.Tensor):
        with torch.no_grad():
            audio_tokens, scale = self.compression_model.encode(audio)
            assert scale is None, &#34;Scaled compression model not supported with LM.&#34;
            return audio_tokens

    def _prepare_tokens_and_attributes(
        self, batch: tp.Tuple[torch.Tensor, tp.List[SegmentWithAttributes]],
        check_synchronization_points: bool = False
    ) -&gt; tp.Tuple[dict, torch.Tensor, torch.Tensor]:
        &#34;&#34;&#34;Prepare input batchs for language model training.

        Args:
            batch (tuple[torch.Tensor, list[SegmentWithAttributes]]): Input batch with audio tensor of shape [B, C, T]
                and corresponding metadata as SegmentWithAttributes (with B items).
            check_synchronization_points (bool): Whether to check for synchronization points slowing down training.
        Returns:
            Condition tensors (dict[str, any]): Preprocessed condition attributes.
            Tokens (torch.Tensor): Audio tokens from compression model, of shape [B, K, T_s],
                with B the batch size, K the number of codebooks, T_s the token timesteps.
            Padding mask (torch.Tensor): Mask with valid positions in the tokens tensor, of shape [B, K, T_s].
        &#34;&#34;&#34;
        if self.model.training:
            warnings.warn(
                &#34;Up to version 1.0.1, the _prepare_tokens_and_attributes was evaluated with `torch.no_grad()`. &#34;
                &#34;This is inconsistent with how model were trained in the MusicGen paper. We removed the &#34;
                &#34;`torch.no_grad()` in version 1.1.0. Small changes to the final performance are expected. &#34;
                &#34;Really sorry about that.&#34;)
        if self._cached_batch_loader is None or self.current_stage != &#34;train&#34;:
            audio, infos = batch
            audio = audio.to(self.device)
            audio_tokens = None
            assert audio.size(0) == len(infos), (
                f&#34;Mismatch between number of items in audio batch ({audio.size(0)})&#34;,
                f&#34; and in metadata ({len(infos)})&#34;
            )
        else:
            audio = None
            # In that case the batch will be a tuple coming from the _cached_batch_writer bit below.
            infos, = batch  # type: ignore
            assert all([isinstance(info, AudioInfo) for info in infos])
            assert all([info.audio_tokens is not None for info in infos])  # type: ignore
            audio_tokens = torch.stack([info.audio_tokens for info in infos]).to(self.device)  # type: ignore
            audio_tokens = audio_tokens.long()
            for info in infos:
                if isinstance(info, MusicInfo):
                    # Careful here, if you want to use this condition_wav (e.b. chroma conditioning),
                    # then you must be using the chroma cache! otherwise the code will try
                    # to use this segment and fail (by that I mean you will see NaN everywhere).
                    info.self_wav = WavCondition(
                        torch.full([1, info.channels, info.total_frames], float(&#39;NaN&#39;)),
                        length=torch.tensor([info.n_frames]),
                        sample_rate=[info.sample_rate],
                        path=[info.meta.path],
                        seek_time=[info.seek_time])
                    dataset = get_dataset_from_loader(self.dataloaders[&#39;original_train&#39;])
                    assert isinstance(dataset, MusicDataset), type(dataset)
                    if dataset.paraphraser is not None and info.description is not None:
                        # Hackingly reapplying paraphraser when using cache.
                        info.description = dataset.paraphraser.sample_paraphrase(
                            info.meta.path, info.description)
        # prepare attributes
        attributes = [info.to_condition_attributes() for info in infos]
        attributes = self.model.cfg_dropout(attributes)
        attributes = self.model.att_dropout(attributes)
        tokenized = self.model.condition_provider.tokenize(attributes)

        # Now we should be synchronization free.
        if self.device == &#34;cuda&#34; and check_synchronization_points:
            torch.cuda.set_sync_debug_mode(&#34;warn&#34;)

        if audio_tokens is None:
            audio_tokens = self._get_audio_tokens(audio)

        with self.autocast:
            condition_tensors = self.model.condition_provider(tokenized)

        # create a padding mask to hold valid vs invalid positions
        padding_mask = torch.ones_like(audio_tokens, dtype=torch.bool, device=audio_tokens.device)
        # replace encodec tokens from padded audio with special_token_id
        if self.cfg.tokens.padding_with_special_token:
            audio_tokens = audio_tokens.clone()
            padding_mask = padding_mask.clone()
            token_sample_rate = self.compression_model.frame_rate
            B, K, T_s = audio_tokens.shape
            for i in range(B):
                n_samples = infos[i].n_frames
                audio_sample_rate = infos[i].sample_rate
                # take the last token generated from actual audio frames (non-padded audio)
                valid_tokens = math.floor(float(n_samples) / audio_sample_rate * token_sample_rate)
                audio_tokens[i, :, valid_tokens:] = self.model.special_token_id
                padding_mask[i, :, valid_tokens:] = 0

        if self.device == &#34;cuda&#34; and check_synchronization_points:
            torch.cuda.set_sync_debug_mode(&#34;default&#34;)

        if self._cached_batch_writer is not None and self.current_stage == &#39;train&#39;:
            assert self._cached_batch_loader is None
            assert audio_tokens is not None
            for info, one_audio_tokens in zip(infos, audio_tokens):
                assert isinstance(info, AudioInfo)
                if isinstance(info, MusicInfo):
                    assert not info.joint_embed, &#34;joint_embed and cache not supported yet.&#34;
                    info.self_wav = None
                assert one_audio_tokens.max() &lt; 2**15, one_audio_tokens.max().item()
                info.audio_tokens = one_audio_tokens.short().cpu()
            self._cached_batch_writer.save(infos)

        return condition_tensors, audio_tokens, padding_mask

    def run_step(self, idx: int, batch: tp.Tuple[torch.Tensor, tp.List[SegmentWithAttributes]], metrics: dict) -&gt; dict:
        &#34;&#34;&#34;Perform one training or valid step on a given batch.&#34;&#34;&#34;
        check_synchronization_points = idx == 1 and self.device == &#39;cuda&#39;

        condition_tensors, audio_tokens, padding_mask = self._prepare_tokens_and_attributes(
            batch, check_synchronization_points)

        self.deadlock_detect.update(&#39;tokens_and_conditions&#39;)

        if check_synchronization_points:
            torch.cuda.set_sync_debug_mode(&#39;warn&#39;)

        with self.autocast:
            style_mask = None
            if hasattr(self.model.condition_provider.conditioners, &#39;self_wav&#39;):
                if isinstance(self.model.condition_provider.conditioners.self_wav, StyleConditioner):
                    style_mask = self.model.condition_provider.conditioners.self_wav.mask

            model_output = self.model.compute_predictions(audio_tokens, [], condition_tensors)  # type: ignore
            logits = model_output.logits
            if style_mask is not None:
                mask = padding_mask &amp; model_output.mask &amp; style_mask
            else:
                mask = padding_mask &amp; model_output.mask
            ce, ce_per_codebook = self._compute_cross_entropy(logits, audio_tokens, mask)
            loss = ce
        self.deadlock_detect.update(&#39;loss&#39;)

        if check_synchronization_points:
            torch.cuda.set_sync_debug_mode(&#39;default&#39;)

        if self.is_training:
            metrics[&#39;lr&#39;] = self.optimizer.param_groups[0][&#39;lr&#39;]
            if self.scaler is not None:
                loss = self.scaler.scale(loss)
            self.deadlock_detect.update(&#39;scale&#39;)
            if self.cfg.fsdp.use:
                loss.backward()
                flashy.distrib.average_tensors(self.model.buffers())
            elif self.cfg.optim.eager_sync:
                with flashy.distrib.eager_sync_model(self.model):
                    loss.backward()
            else:
                # this should always be slower but can be useful
                # for weird use cases like multiple backwards.
                loss.backward()
                flashy.distrib.sync_model(self.model)
            self.deadlock_detect.update(&#39;backward&#39;)

            if self.scaler is not None:
                self.scaler.unscale_(self.optimizer)
            if self.cfg.optim.max_norm:
                if self.cfg.fsdp.use:
                    metrics[&#39;grad_norm&#39;] = self.model.clip_grad_norm_(self.cfg.optim.max_norm)  # type: ignore
                else:
                    metrics[&#39;grad_norm&#39;] = torch.nn.utils.clip_grad_norm_(
                        self.model.parameters(), self.cfg.optim.max_norm
                    )
            if self.scaler is None:
                self.optimizer.step()
            else:
                self.scaler.step(self.optimizer)
                self.scaler.update()
            if self.lr_scheduler:
                self.lr_scheduler.step()
            self.optimizer.zero_grad()
            self.deadlock_detect.update(&#39;optim&#39;)
            if self.scaler is not None:
                scale = self.scaler.get_scale()
                metrics[&#39;grad_scale&#39;] = scale
            if not loss.isfinite().all():
                raise RuntimeError(&#34;Model probably diverged.&#34;)

        metrics[&#39;ce&#39;] = ce
        metrics[&#39;ppl&#39;] = torch.exp(ce)
        for k, ce_q in enumerate(ce_per_codebook):
            metrics[f&#39;ce_q{k + 1}&#39;] = ce_q
            metrics[f&#39;ppl_q{k + 1}&#39;] = torch.exp(ce_q)

        return metrics

    @torch.no_grad()
    def run_generate_step(self, batch: tp.Tuple[torch.Tensor, tp.List[SegmentWithAttributes]],
                          gen_duration: float, prompt_duration: tp.Optional[float] = None,
                          remove_text_conditioning: bool = False,
                          ) -&gt; dict:
        &#34;&#34;&#34;Run generate step on a batch of optional audio tensor and corresponding attributes.

        Args:
            batch (tuple[torch.Tensor, list[SegmentWithAttributes]]):
            use_prompt (bool): Whether to do audio continuation generation with prompt from audio batch.
            gen_duration (float): Target audio duration for the generation.
            prompt_duration (float, optional): Duration for the audio prompt to use for continuation.
        Returns:
            gen_outputs (dict): Generation outputs, consisting in audio, audio tokens from both the generation
                and the prompt along with additional information.
        &#34;&#34;&#34;
        bench_start = time.time()
        audio, meta = batch
        assert audio.size(0) == len(meta), (
            f&#34;Mismatch between number of items in audio batch ({audio.size(0)})&#34;,
            f&#34; and in metadata ({len(meta)})&#34;
        )
        # prepare attributes
        attributes = [x.to_condition_attributes() for x in meta]
        if remove_text_conditioning:
            attributes = _drop_description_condition(attributes)

        # prepare audio prompt
        if prompt_duration is None:
            prompt_audio = None
        else:
            assert prompt_duration &lt; gen_duration, &#34;Prompt duration must be lower than target generation duration&#34;
            prompt_audio_frames = int(prompt_duration * self.compression_model.sample_rate)
            prompt_audio = audio[..., :prompt_audio_frames]

        # get audio tokens from compression model
        if prompt_audio is None or prompt_audio.nelement() == 0:
            num_samples = len(attributes)
            prompt_tokens = None
        else:
            num_samples = None
            prompt_audio = prompt_audio.to(self.device)
            prompt_tokens, scale = self.compression_model.encode(prompt_audio)
            assert scale is None, &#34;Compression model in MusicGen should not require rescaling.&#34;

        # generate by sampling from the LM
        with self.autocast:
            total_gen_len = math.ceil(gen_duration * self.compression_model.frame_rate)
            gen_tokens = self.model.generate(
                prompt_tokens, attributes, max_gen_len=total_gen_len,
                num_samples=num_samples, **self.generation_params)

        # generate audio from tokens
        assert gen_tokens.dim() == 3
        gen_audio = self.compression_model.decode(gen_tokens, None)

        bench_end = time.time()
        gen_outputs = {
            &#39;rtf&#39;: (bench_end - bench_start) / gen_duration,
            &#39;ref_audio&#39;: audio,
            &#39;gen_audio&#39;: gen_audio,
            &#39;gen_tokens&#39;: gen_tokens,
            &#39;prompt_audio&#39;: prompt_audio,
            &#39;prompt_tokens&#39;: prompt_tokens,
        }
        return gen_outputs

    def generate_audio(self) -&gt; dict:
        &#34;&#34;&#34;Audio generation stage.&#34;&#34;&#34;
        generate_stage_name = f&#39;{self.current_stage}&#39;
        sample_manager = SampleManager(self.xp)
        self.logger.info(f&#34;Generating samples in {sample_manager.base_folder}&#34;)
        loader = self.dataloaders[&#39;generate&#39;]
        updates = len(loader)
        lp = self.log_progress(generate_stage_name, loader, total=updates, updates=self.log_updates)

        dataset = get_dataset_from_loader(loader)
        dataset_duration = dataset.segment_duration
        assert dataset_duration is not None
        assert isinstance(dataset, AudioDataset)
        target_duration = self.cfg.generate.lm.gen_duration
        prompt_duration = self.cfg.generate.lm.prompt_duration
        if target_duration is None:
            target_duration = dataset_duration
        if prompt_duration is None:
            prompt_duration = dataset_duration / 4
        assert prompt_duration &lt; dataset_duration, (
            f&#34;Specified prompt duration ({prompt_duration}s) is longer&#34;,
            f&#34; than reference audio duration ({dataset_duration}s)&#34;
        )

        def get_hydrated_conditions(meta: tp.List[SegmentWithAttributes]):
            hydrated_conditions = []
            for sample in [x.to_condition_attributes() for x in meta]:
                cond_dict = {}
                for cond_type in sample.__annotations__.keys():
                    for cond_key, cond_val in getattr(sample, cond_type).items():
                        if cond_key not in self.model.condition_provider.conditioners.keys():
                            continue
                        if is_jsonable(cond_val):
                            cond_dict[cond_key] = cond_val
                        elif isinstance(cond_val, WavCondition):
                            cond_dict[cond_key] = cond_val.path
                        elif isinstance(cond_val, JointEmbedCondition):
                            cond_dict[cond_key] = cond_val.text  # only support text at inference for now
                        else:
                            # if we reached this point, it is not clear how to log the condition
                            # so we just log the type.
                            cond_dict[cond_key] = str(type(cond_val))
                            continue
                hydrated_conditions.append(cond_dict)
            return hydrated_conditions

        metrics: dict = {}
        average = flashy.averager()
        for batch in lp:
            audio, meta = batch
            # metadata for sample manager
            hydrated_conditions = get_hydrated_conditions(meta)
            sample_generation_params = {
                **{f&#39;classifier_free_guidance_{k}&#39;: v for k, v in self.cfg.classifier_free_guidance.items()},
                **self.generation_params
            }
            if self.cfg.generate.lm.unprompted_samples:
                if self.cfg.generate.lm.gen_gt_samples:
                    # get the ground truth instead of generation
                    self.logger.warn(
                        &#34;Use ground truth instead of audio generation as generate.lm.gen_gt_samples=true&#34;)
                    gen_unprompted_audio = audio
                    rtf = 1.
                else:
                    gen_unprompted_outputs = self.run_generate_step(
                        batch, gen_duration=target_duration, prompt_duration=None)
                    gen_unprompted_audio = gen_unprompted_outputs[&#39;gen_audio&#39;].cpu()
                    rtf = gen_unprompted_outputs[&#39;rtf&#39;]
                sample_manager.add_samples(
                    gen_unprompted_audio, self.epoch, hydrated_conditions,
                    ground_truth_wavs=audio, generation_args=sample_generation_params)

            if self.cfg.generate.lm.prompted_samples:
                gen_outputs = self.run_generate_step(
                    batch, gen_duration=target_duration, prompt_duration=prompt_duration)
                gen_audio = gen_outputs[&#39;gen_audio&#39;].cpu()
                prompt_audio = gen_outputs[&#39;prompt_audio&#39;].cpu()
                sample_manager.add_samples(
                    gen_audio, self.epoch, hydrated_conditions,
                    prompt_wavs=prompt_audio, ground_truth_wavs=audio,
                    generation_args=sample_generation_params)
            if self.cfg.generate.lm.no_text_conditioning:
                gen_outputs = self.run_generate_step(
                    batch, gen_duration=target_duration, prompt_duration=None,
                    remove_text_conditioning=self.cfg.generate.lm.no_text_conditioning)
                gen_audio = gen_outputs[&#39;gen_audio&#39;].cpu()
                rtf = gen_outputs[&#39;rtf&#39;]
                # Here, the prompt is the original audio provided for the style conditioning
                prompt_audio = gen_outputs[&#39;ref_audio&#39;].cpu()
                sample_manager.add_samples(
                    gen_audio, self.epoch, hydrated_conditions,
                    prompt_wavs=prompt_audio, ground_truth_wavs=audio,
                    generation_args=sample_generation_params)

            metrics[&#39;rtf&#39;] = rtf
            metrics = average(metrics)

        flashy.distrib.barrier()
        return metrics

    def generate(self) -&gt; dict:
        &#34;&#34;&#34;Generate stage.&#34;&#34;&#34;
        self.model.eval()
        with torch.no_grad():
            return self.generate_audio()

    def run_epoch(self):
        if self.cfg.cache.write:
            if ((self.epoch - 1) % self.cfg.cache.write_num_shards) != self.cfg.cache.write_shard:
                return
        super().run_epoch()

    def train(self):
        &#34;&#34;&#34;Train stage.
        &#34;&#34;&#34;
        if self._cached_batch_writer is not None:
            self._cached_batch_writer.start_epoch(self.epoch)
        if self._cached_batch_loader is None:
            dataset = get_dataset_from_loader(self.dataloaders[&#39;train&#39;])
            assert isinstance(dataset, AudioDataset)
            dataset.current_epoch = self.epoch
        else:
            self._cached_batch_loader.start_epoch(self.epoch)
        return super().train()

    def evaluate_audio_generation(self) -&gt; dict:
        &#34;&#34;&#34;Evaluate audio generation with off-the-shelf metrics.&#34;&#34;&#34;
        evaluate_stage_name = f&#39;{self.current_stage}_generation&#39;
        # instantiate evaluation metrics, if at least one metric is defined, run audio generation evaluation
        fad: tp.Optional[eval_metrics.FrechetAudioDistanceMetric] = None
        kldiv: tp.Optional[eval_metrics.KLDivergenceMetric] = None
        text_consistency: tp.Optional[eval_metrics.TextConsistencyMetric] = None
        chroma_cosine: tp.Optional[eval_metrics.ChromaCosineSimilarityMetric] = None
        should_run_eval = False
        eval_chroma_wavs: tp.Optional[torch.Tensor] = None
        if self.cfg.evaluate.metrics.fad:
            fad = builders.get_fad(self.cfg.metrics.fad).to(self.device)
            should_run_eval = True
        if self.cfg.evaluate.metrics.kld:
            kldiv = builders.get_kldiv(self.cfg.metrics.kld).to(self.device)
            should_run_eval = True
        if self.cfg.evaluate.metrics.text_consistency:
            text_consistency = builders.get_text_consistency(self.cfg.metrics.text_consistency).to(self.device)
            should_run_eval = True
        if self.cfg.evaluate.metrics.chroma_cosine:
            chroma_cosine = builders.get_chroma_cosine_similarity(self.cfg.metrics.chroma_cosine).to(self.device)
            # if we have predefind wavs for chroma we should purge them for computing the cosine metric
            has_predefined_eval_chromas = &#39;self_wav&#39; in self.model.condition_provider.conditioners and \
                                          self.model.condition_provider.conditioners[&#39;self_wav&#39;].has_eval_wavs()
            if has_predefined_eval_chromas:
                warn_once(self.logger, &#34;Attempting to run cosine eval for config with pre-defined eval chromas! &#34;
                                       &#39;Resetting eval chromas to None for evaluation.&#39;)
                eval_chroma_wavs = self.model.condition_provider.conditioners.self_wav.eval_wavs  # type: ignore
                self.model.condition_provider.conditioners.self_wav.reset_eval_wavs(None)  # type: ignore
            should_run_eval = True

        def get_compressed_audio(audio: torch.Tensor) -&gt; torch.Tensor:
            audio_tokens, scale = self.compression_model.encode(audio.to(self.device))
            compressed_audio = self.compression_model.decode(audio_tokens, scale)
            return compressed_audio[..., :audio.shape[-1]]

        metrics: dict = {}
        if should_run_eval:
            loader = self.dataloaders[&#39;evaluate&#39;]
            updates = len(loader)
            lp = self.log_progress(f&#39;{evaluate_stage_name} inference&#39;, loader, total=updates, updates=self.log_updates)
            average = flashy.averager()
            dataset = get_dataset_from_loader(loader)
            assert isinstance(dataset, AudioDataset)
            self.logger.info(f&#34;Computing evaluation metrics on {len(dataset)} samples&#34;)

            for idx, batch in enumerate(lp):
                audio, meta = batch
                assert all([self.cfg.sample_rate == m.sample_rate for m in meta])

                target_duration = audio.shape[-1] / self.cfg.sample_rate
                if self.cfg.evaluate.fixed_generation_duration:
                    target_duration = self.cfg.evaluate.fixed_generation_duration

                gen_outputs = self.run_generate_step(
                    batch, gen_duration=target_duration,
                    remove_text_conditioning=self.cfg.evaluate.get(&#39;remove_text_conditioning&#39;, False)
                )
                y_pred = gen_outputs[&#39;gen_audio&#39;].detach()
                y_pred = y_pred[..., :audio.shape[-1]]

                normalize_kwargs = dict(self.cfg.generate.audio)
                normalize_kwargs.pop(&#39;format&#39;, None)
                y_pred = torch.stack([normalize_audio(w, **normalize_kwargs) for w in y_pred], dim=0).cpu()
                y = audio.cpu()  # should already be on CPU but just in case
                sizes = torch.tensor([m.n_frames for m in meta])  # actual sizes without padding
                sample_rates = torch.tensor([m.sample_rate for m in meta])  # sample rates for audio samples
                audio_stems = [Path(m.meta.path).stem + f&#34;_{m.seek_time}&#34; for m in meta]

                if fad is not None:
                    if self.cfg.metrics.fad.use_gt:
                        y_pred = get_compressed_audio(y).cpu()
                    fad.update(y_pred, y, sizes, sample_rates, audio_stems)
                if kldiv is not None:
                    if self.cfg.metrics.kld.use_gt:
                        y_pred = get_compressed_audio(y).cpu()
                    kldiv.update(y_pred, y, sizes, sample_rates)
                if text_consistency is not None:
                    texts = [m.description for m in meta]
                    if self.cfg.metrics.text_consistency.use_gt:
                        y_pred = y
                    text_consistency.update(y_pred, texts, sizes, sample_rates)
                if chroma_cosine is not None:
                    if self.cfg.metrics.chroma_cosine.use_gt:
                        y_pred = get_compressed_audio(y).cpu()
                    chroma_cosine.update(y_pred, y, sizes, sample_rates)
                    # restore chroma conditioner&#39;s eval chroma wavs
                    if eval_chroma_wavs is not None:
                        self.model.condition_provider.conditioners[&#39;self_wav&#39;].reset_eval_wavs(eval_chroma_wavs)

            flashy.distrib.barrier()
            if fad is not None:
                metrics[&#39;fad&#39;] = fad.compute()
            if kldiv is not None:
                kld_metrics = kldiv.compute()
                metrics.update(kld_metrics)
            if text_consistency is not None:
                metrics[&#39;text_consistency&#39;] = text_consistency.compute()
            if chroma_cosine is not None:
                metrics[&#39;chroma_cosine&#39;] = chroma_cosine.compute()
            metrics = average(metrics)
            metrics = flashy.distrib.average_metrics(metrics, len(loader))

        return metrics

    def evaluate(self) -&gt; dict:
        &#34;&#34;&#34;Evaluate stage.&#34;&#34;&#34;
        self.model.eval()
        with torch.no_grad():
            metrics: dict = {}
            if self.cfg.evaluate.metrics.base:
                metrics.update(self.common_train_valid(&#39;evaluate&#39;))
            gen_metrics = self.evaluate_audio_generation()
            return {**metrics, **gen_metrics}</code></pre>
</details>
<div class="desc"><p>Solver for MusicGen training task.</p>
<p>Used in: <a href="https://arxiv.org/abs/2306.05284">https://arxiv.org/abs/2306.05284</a></p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="audiocraft.solvers.base.StandardSolver" href="base.html#audiocraft.solvers.base.StandardSolver">StandardSolver</a></li>
<li>abc.ABC</li>
<li>flashy.solver.BaseSolver</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="audiocraft.solvers.audiogen.AudioGenSolver" href="audiogen.html#audiocraft.solvers.audiogen.AudioGenSolver">AudioGenSolver</a></li>
<li><a title="audiocraft.solvers.jasco.JascoSolver" href="jasco.html#audiocraft.solvers.jasco.JascoSolver">JascoSolver</a></li>
<li><a title="audiocraft.solvers.magnet.MagnetSolver" href="magnet.html#audiocraft.solvers.magnet.MagnetSolver">MagnetSolver</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="audiocraft.solvers.musicgen.MusicGenSolver.DATASET_TYPE"><code class="name">var <span class="ident">DATASET_TYPE</span> : <a title="audiocraft.solvers.builders.DatasetType" href="builders.html#audiocraft.solvers.builders.DatasetType">DatasetType</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="audiocraft.solvers.musicgen.MusicGenSolver.get_eval_solver_from_sig"><code class="name flex">
<span>def <span class="ident">get_eval_solver_from_sig</span></span>(<span>sig: str,<br>dtype: str | None = None,<br>device: str | None = None,<br>autocast: bool = True,<br>batch_size: int | None = None,<br>override_cfg: dict | omegaconf.dictconfig.DictConfig | None = None,<br>**kwargs)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def get_eval_solver_from_sig(sig: str, dtype: tp.Optional[str] = None,
                             device: tp.Optional[str] = None, autocast: bool = True,
                             batch_size: tp.Optional[int] = None,
                             override_cfg: tp.Optional[tp.Union[dict, omegaconf.DictConfig]] = None,
                             **kwargs):
    &#34;&#34;&#34;Mostly a convenience function around magma.train.get_solver_from_sig,
    populating all the proper param, deactivating EMA, FSDP, loading the best state,
    basically all you need to get a solver ready to &#34;play&#34; with in single GPU mode
    and with minimal memory overhead.

    Args:
        sig (str): signature to load.
        dtype (str or None): potential dtype, as a string, i.e. &#39;float16&#39;.
        device (str or None): potential device, as a string, i.e. &#39;cuda&#39;.
        override_cfg (dict or omegaconf.DictConfig or None): potential device, as a string, i.e. &#39;cuda&#39;.
    &#34;&#34;&#34;
    from audiocraft import train
    our_override_cfg: tp.Dict[str, tp.Any] = {&#39;optim&#39;: {&#39;ema&#39;: {&#39;use&#39;: False}}}
    our_override_cfg[&#39;autocast&#39;] = autocast
    if dtype is not None:
        our_override_cfg[&#39;dtype&#39;] = dtype
    if device is not None:
        our_override_cfg[&#39;device&#39;] = device
    if batch_size is not None:
        our_override_cfg[&#39;dataset&#39;] = {&#39;batch_size&#39;: batch_size}
    if override_cfg is None:
        override_cfg = {}
    override_cfg = omegaconf.OmegaConf.merge(
        omegaconf.DictConfig(override_cfg), omegaconf.DictConfig(our_override_cfg))  # type: ignore
    solver = train.get_solver_from_sig(
        sig, override_cfg=override_cfg,
        load_best=True, disable_fsdp=True,
        ignore_state_keys=[&#39;optimizer&#39;, &#39;ema&#39;], **kwargs)
    solver.model.eval()
    return solver</code></pre>
</details>
<div class="desc"><p>Mostly a convenience function around magma.train.get_solver_from_sig,
populating all the proper param, deactivating EMA, FSDP, loading the best state,
basically all you need to get a solver ready to "play" with in single GPU mode
and with minimal memory overhead.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>sig</code></strong> :&ensp;<code>str</code></dt>
<dd>signature to load.</dd>
<dt><strong><code>dtype</code></strong> :&ensp;<code>str</code> or <code>None</code></dt>
<dd>potential dtype, as a string, i.e. 'float16'.</dd>
<dt><strong><code>device</code></strong> :&ensp;<code>str</code> or <code>None</code></dt>
<dd>potential device, as a string, i.e. 'cuda'.</dd>
<dt><strong><code>override_cfg</code></strong> :&ensp;<code>dict</code> or <code>omegaconf.DictConfig</code> or <code>None</code></dt>
<dd>potential device, as a string, i.e. 'cuda'.</dd>
</dl></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="audiocraft.solvers.musicgen.MusicGenSolver.build_dataloaders"><code class="name flex">
<span>def <span class="ident">build_dataloaders</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build_dataloaders(self) -&gt; None:
    &#34;&#34;&#34;Instantiate audio dataloaders for each stage.&#34;&#34;&#34;
    self.dataloaders = builders.get_audio_datasets(self.cfg, dataset_type=self.DATASET_TYPE)</code></pre>
</details>
<div class="desc"><p>Instantiate audio dataloaders for each stage.</p></div>
</dd>
<dt id="audiocraft.solvers.musicgen.MusicGenSolver.build_model"><code class="name flex">
<span>def <span class="ident">build_model</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build_model(self) -&gt; None:
    &#34;&#34;&#34;Instantiate models and optimizer.&#34;&#34;&#34;
    # we can potentially not use all quantizers with which the EnCodec model was trained
    # (e.g. we trained the model with quantizers dropout)
    self.compression_model = CompressionSolver.wrapped_model_from_checkpoint(
        self.cfg, self.cfg.compression_model_checkpoint, device=self.device)
    assert self.compression_model.sample_rate == self.cfg.sample_rate, (
        f&#34;Compression model sample rate is {self.compression_model.sample_rate} but &#34;
        f&#34;Solver sample rate is {self.cfg.sample_rate}.&#34;
        )
    # ensure we have matching configuration between LM and compression model
    assert self.cfg.transformer_lm.card == self.compression_model.cardinality, (
        &#34;Cardinalities of the LM and compression model don&#39;t match: &#34;,
        f&#34;LM cardinality is {self.cfg.transformer_lm.card} vs &#34;,
        f&#34;compression model cardinality is {self.compression_model.cardinality}&#34;
    )
    assert self.cfg.transformer_lm.n_q == self.compression_model.num_codebooks, (
        &#34;Numbers of codebooks of the LM and compression models don&#39;t match: &#34;,
        f&#34;LM number of codebooks is {self.cfg.transformer_lm.n_q} vs &#34;,
        f&#34;compression model numer of codebooks is {self.compression_model.num_codebooks}&#34;
    )
    self.logger.info(&#34;Compression model has %d codebooks with %d cardinality, and a framerate of %d&#34;,
                     self.compression_model.num_codebooks, self.compression_model.cardinality,
                     self.compression_model.frame_rate)
    # instantiate LM model
    self.model: tp.Union[models.LMModel, models.FlowMatchingModel] = models.builders.get_lm_model(
        self.cfg).to(self.device)

    # initialize optimization
    self.initialize_optimization()</code></pre>
</details>
<div class="desc"><p>Instantiate models and optimizer.</p></div>
</dd>
<dt id="audiocraft.solvers.musicgen.MusicGenSolver.evaluate_audio_generation"><code class="name flex">
<span>def <span class="ident">evaluate_audio_generation</span></span>(<span>self) ‑> dict</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def evaluate_audio_generation(self) -&gt; dict:
    &#34;&#34;&#34;Evaluate audio generation with off-the-shelf metrics.&#34;&#34;&#34;
    evaluate_stage_name = f&#39;{self.current_stage}_generation&#39;
    # instantiate evaluation metrics, if at least one metric is defined, run audio generation evaluation
    fad: tp.Optional[eval_metrics.FrechetAudioDistanceMetric] = None
    kldiv: tp.Optional[eval_metrics.KLDivergenceMetric] = None
    text_consistency: tp.Optional[eval_metrics.TextConsistencyMetric] = None
    chroma_cosine: tp.Optional[eval_metrics.ChromaCosineSimilarityMetric] = None
    should_run_eval = False
    eval_chroma_wavs: tp.Optional[torch.Tensor] = None
    if self.cfg.evaluate.metrics.fad:
        fad = builders.get_fad(self.cfg.metrics.fad).to(self.device)
        should_run_eval = True
    if self.cfg.evaluate.metrics.kld:
        kldiv = builders.get_kldiv(self.cfg.metrics.kld).to(self.device)
        should_run_eval = True
    if self.cfg.evaluate.metrics.text_consistency:
        text_consistency = builders.get_text_consistency(self.cfg.metrics.text_consistency).to(self.device)
        should_run_eval = True
    if self.cfg.evaluate.metrics.chroma_cosine:
        chroma_cosine = builders.get_chroma_cosine_similarity(self.cfg.metrics.chroma_cosine).to(self.device)
        # if we have predefind wavs for chroma we should purge them for computing the cosine metric
        has_predefined_eval_chromas = &#39;self_wav&#39; in self.model.condition_provider.conditioners and \
                                      self.model.condition_provider.conditioners[&#39;self_wav&#39;].has_eval_wavs()
        if has_predefined_eval_chromas:
            warn_once(self.logger, &#34;Attempting to run cosine eval for config with pre-defined eval chromas! &#34;
                                   &#39;Resetting eval chromas to None for evaluation.&#39;)
            eval_chroma_wavs = self.model.condition_provider.conditioners.self_wav.eval_wavs  # type: ignore
            self.model.condition_provider.conditioners.self_wav.reset_eval_wavs(None)  # type: ignore
        should_run_eval = True

    def get_compressed_audio(audio: torch.Tensor) -&gt; torch.Tensor:
        audio_tokens, scale = self.compression_model.encode(audio.to(self.device))
        compressed_audio = self.compression_model.decode(audio_tokens, scale)
        return compressed_audio[..., :audio.shape[-1]]

    metrics: dict = {}
    if should_run_eval:
        loader = self.dataloaders[&#39;evaluate&#39;]
        updates = len(loader)
        lp = self.log_progress(f&#39;{evaluate_stage_name} inference&#39;, loader, total=updates, updates=self.log_updates)
        average = flashy.averager()
        dataset = get_dataset_from_loader(loader)
        assert isinstance(dataset, AudioDataset)
        self.logger.info(f&#34;Computing evaluation metrics on {len(dataset)} samples&#34;)

        for idx, batch in enumerate(lp):
            audio, meta = batch
            assert all([self.cfg.sample_rate == m.sample_rate for m in meta])

            target_duration = audio.shape[-1] / self.cfg.sample_rate
            if self.cfg.evaluate.fixed_generation_duration:
                target_duration = self.cfg.evaluate.fixed_generation_duration

            gen_outputs = self.run_generate_step(
                batch, gen_duration=target_duration,
                remove_text_conditioning=self.cfg.evaluate.get(&#39;remove_text_conditioning&#39;, False)
            )
            y_pred = gen_outputs[&#39;gen_audio&#39;].detach()
            y_pred = y_pred[..., :audio.shape[-1]]

            normalize_kwargs = dict(self.cfg.generate.audio)
            normalize_kwargs.pop(&#39;format&#39;, None)
            y_pred = torch.stack([normalize_audio(w, **normalize_kwargs) for w in y_pred], dim=0).cpu()
            y = audio.cpu()  # should already be on CPU but just in case
            sizes = torch.tensor([m.n_frames for m in meta])  # actual sizes without padding
            sample_rates = torch.tensor([m.sample_rate for m in meta])  # sample rates for audio samples
            audio_stems = [Path(m.meta.path).stem + f&#34;_{m.seek_time}&#34; for m in meta]

            if fad is not None:
                if self.cfg.metrics.fad.use_gt:
                    y_pred = get_compressed_audio(y).cpu()
                fad.update(y_pred, y, sizes, sample_rates, audio_stems)
            if kldiv is not None:
                if self.cfg.metrics.kld.use_gt:
                    y_pred = get_compressed_audio(y).cpu()
                kldiv.update(y_pred, y, sizes, sample_rates)
            if text_consistency is not None:
                texts = [m.description for m in meta]
                if self.cfg.metrics.text_consistency.use_gt:
                    y_pred = y
                text_consistency.update(y_pred, texts, sizes, sample_rates)
            if chroma_cosine is not None:
                if self.cfg.metrics.chroma_cosine.use_gt:
                    y_pred = get_compressed_audio(y).cpu()
                chroma_cosine.update(y_pred, y, sizes, sample_rates)
                # restore chroma conditioner&#39;s eval chroma wavs
                if eval_chroma_wavs is not None:
                    self.model.condition_provider.conditioners[&#39;self_wav&#39;].reset_eval_wavs(eval_chroma_wavs)

        flashy.distrib.barrier()
        if fad is not None:
            metrics[&#39;fad&#39;] = fad.compute()
        if kldiv is not None:
            kld_metrics = kldiv.compute()
            metrics.update(kld_metrics)
        if text_consistency is not None:
            metrics[&#39;text_consistency&#39;] = text_consistency.compute()
        if chroma_cosine is not None:
            metrics[&#39;chroma_cosine&#39;] = chroma_cosine.compute()
        metrics = average(metrics)
        metrics = flashy.distrib.average_metrics(metrics, len(loader))

    return metrics</code></pre>
</details>
<div class="desc"><p>Evaluate audio generation with off-the-shelf metrics.</p></div>
</dd>
<dt id="audiocraft.solvers.musicgen.MusicGenSolver.generate_audio"><code class="name flex">
<span>def <span class="ident">generate_audio</span></span>(<span>self) ‑> dict</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_audio(self) -&gt; dict:
    &#34;&#34;&#34;Audio generation stage.&#34;&#34;&#34;
    generate_stage_name = f&#39;{self.current_stage}&#39;
    sample_manager = SampleManager(self.xp)
    self.logger.info(f&#34;Generating samples in {sample_manager.base_folder}&#34;)
    loader = self.dataloaders[&#39;generate&#39;]
    updates = len(loader)
    lp = self.log_progress(generate_stage_name, loader, total=updates, updates=self.log_updates)

    dataset = get_dataset_from_loader(loader)
    dataset_duration = dataset.segment_duration
    assert dataset_duration is not None
    assert isinstance(dataset, AudioDataset)
    target_duration = self.cfg.generate.lm.gen_duration
    prompt_duration = self.cfg.generate.lm.prompt_duration
    if target_duration is None:
        target_duration = dataset_duration
    if prompt_duration is None:
        prompt_duration = dataset_duration / 4
    assert prompt_duration &lt; dataset_duration, (
        f&#34;Specified prompt duration ({prompt_duration}s) is longer&#34;,
        f&#34; than reference audio duration ({dataset_duration}s)&#34;
    )

    def get_hydrated_conditions(meta: tp.List[SegmentWithAttributes]):
        hydrated_conditions = []
        for sample in [x.to_condition_attributes() for x in meta]:
            cond_dict = {}
            for cond_type in sample.__annotations__.keys():
                for cond_key, cond_val in getattr(sample, cond_type).items():
                    if cond_key not in self.model.condition_provider.conditioners.keys():
                        continue
                    if is_jsonable(cond_val):
                        cond_dict[cond_key] = cond_val
                    elif isinstance(cond_val, WavCondition):
                        cond_dict[cond_key] = cond_val.path
                    elif isinstance(cond_val, JointEmbedCondition):
                        cond_dict[cond_key] = cond_val.text  # only support text at inference for now
                    else:
                        # if we reached this point, it is not clear how to log the condition
                        # so we just log the type.
                        cond_dict[cond_key] = str(type(cond_val))
                        continue
            hydrated_conditions.append(cond_dict)
        return hydrated_conditions

    metrics: dict = {}
    average = flashy.averager()
    for batch in lp:
        audio, meta = batch
        # metadata for sample manager
        hydrated_conditions = get_hydrated_conditions(meta)
        sample_generation_params = {
            **{f&#39;classifier_free_guidance_{k}&#39;: v for k, v in self.cfg.classifier_free_guidance.items()},
            **self.generation_params
        }
        if self.cfg.generate.lm.unprompted_samples:
            if self.cfg.generate.lm.gen_gt_samples:
                # get the ground truth instead of generation
                self.logger.warn(
                    &#34;Use ground truth instead of audio generation as generate.lm.gen_gt_samples=true&#34;)
                gen_unprompted_audio = audio
                rtf = 1.
            else:
                gen_unprompted_outputs = self.run_generate_step(
                    batch, gen_duration=target_duration, prompt_duration=None)
                gen_unprompted_audio = gen_unprompted_outputs[&#39;gen_audio&#39;].cpu()
                rtf = gen_unprompted_outputs[&#39;rtf&#39;]
            sample_manager.add_samples(
                gen_unprompted_audio, self.epoch, hydrated_conditions,
                ground_truth_wavs=audio, generation_args=sample_generation_params)

        if self.cfg.generate.lm.prompted_samples:
            gen_outputs = self.run_generate_step(
                batch, gen_duration=target_duration, prompt_duration=prompt_duration)
            gen_audio = gen_outputs[&#39;gen_audio&#39;].cpu()
            prompt_audio = gen_outputs[&#39;prompt_audio&#39;].cpu()
            sample_manager.add_samples(
                gen_audio, self.epoch, hydrated_conditions,
                prompt_wavs=prompt_audio, ground_truth_wavs=audio,
                generation_args=sample_generation_params)
        if self.cfg.generate.lm.no_text_conditioning:
            gen_outputs = self.run_generate_step(
                batch, gen_duration=target_duration, prompt_duration=None,
                remove_text_conditioning=self.cfg.generate.lm.no_text_conditioning)
            gen_audio = gen_outputs[&#39;gen_audio&#39;].cpu()
            rtf = gen_outputs[&#39;rtf&#39;]
            # Here, the prompt is the original audio provided for the style conditioning
            prompt_audio = gen_outputs[&#39;ref_audio&#39;].cpu()
            sample_manager.add_samples(
                gen_audio, self.epoch, hydrated_conditions,
                prompt_wavs=prompt_audio, ground_truth_wavs=audio,
                generation_args=sample_generation_params)

        metrics[&#39;rtf&#39;] = rtf
        metrics = average(metrics)

    flashy.distrib.barrier()
    return metrics</code></pre>
</details>
<div class="desc"><p>Audio generation stage.</p></div>
</dd>
<dt id="audiocraft.solvers.musicgen.MusicGenSolver.get_formatter"><code class="name flex">
<span>def <span class="ident">get_formatter</span></span>(<span>self, stage_name: str) ‑> flashy.formatter.Formatter</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_formatter(self, stage_name: str) -&gt; flashy.Formatter:
    return flashy.Formatter({
        &#39;lr&#39;: &#39;.2E&#39;,
        &#39;ce&#39;: &#39;.3f&#39;,
        &#39;ppl&#39;: &#39;.3f&#39;,
        &#39;grad_norm&#39;: &#39;.3E&#39;,
    }, exclude_keys=[&#39;ce_q*&#39;, &#39;ppl_q*&#39;])</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="audiocraft.solvers.musicgen.MusicGenSolver.initialize_optimization"><code class="name flex">
<span>def <span class="ident">initialize_optimization</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def initialize_optimization(self) -&gt; None:
    if self.cfg.fsdp.use:
        assert not self.cfg.autocast, &#34;Cannot use autocast with fsdp&#34;
        self.model = self.wrap_with_fsdp(self.model)
    self.register_ema(&#39;model&#39;)
    # initialize optimization
    self.optimizer = builders.get_optimizer(builders.get_optim_parameter_groups(self.model), self.cfg.optim)
    self.lr_scheduler = builders.get_lr_scheduler(self.optimizer, self.cfg.schedule, self.total_updates)
    self.register_stateful(&#39;model&#39;, &#39;optimizer&#39;, &#39;lr_scheduler&#39;)
    self.register_best_state(&#39;model&#39;)
    self.autocast_dtype = {
        &#39;float16&#39;: torch.float16, &#39;bfloat16&#39;: torch.bfloat16
    }[self.cfg.autocast_dtype]
    self.scaler: tp.Optional[torch.cuda.amp.GradScaler] = None
    if self.cfg.fsdp.use:
        need_scaler = self.cfg.fsdp.param_dtype == &#39;float16&#39;
    else:
        need_scaler = self.cfg.autocast and self.autocast_dtype is torch.float16
    if need_scaler:
        if self.cfg.fsdp.use:
            from torch.distributed.fsdp.sharded_grad_scaler import ShardedGradScaler
            self.scaler = ShardedGradScaler()  # type: ignore
        else:
            self.scaler = torch.cuda.amp.GradScaler()
        self.register_stateful(&#39;scaler&#39;)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="audiocraft.solvers.musicgen.MusicGenSolver.load_from_pretrained"><code class="name flex">
<span>def <span class="ident">load_from_pretrained</span></span>(<span>self, name: str)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_from_pretrained(self, name: str):
    # TODO: support native HF versions of MusicGen.
    lm_pkg = models.loaders.load_lm_model_ckpt(name)
    state: dict = {
        &#39;best_state&#39;: {
            &#39;model&#39;: lm_pkg[&#39;best_state&#39;],
        },
    }
    return state</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="audiocraft.solvers.musicgen.MusicGenSolver.load_state_dict"><code class="name flex">
<span>def <span class="ident">load_state_dict</span></span>(<span>self, state: dict) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_state_dict(self, state: dict) -&gt; None:
    if &#39;condition_provider&#39; in state:
        model_state = state[&#39;model&#39;]
        condition_provider_state = state.pop(&#39;condition_provider&#39;)
        prefix = &#39;condition_provider.&#39;
        for key, value in condition_provider_state.items():
            key = prefix + key
            assert key not in model_state
            model_state[key] = value
    if &#39;compression_model&#39; in state:
        # We used to store the `compression_model` state in the checkpoint, however
        # this is in general not needed, as the compression model should always be readable
        # from the original `cfg.compression_model_checkpoint` location.
        compression_model_state = state.pop(&#39;compression_model&#39;)
        before_hash = model_hash(self.compression_model)
        self.compression_model.load_state_dict(compression_model_state)
        after_hash = model_hash(self.compression_model)
        if before_hash != after_hash:
            raise RuntimeError(
                &#34;The compression model state inside the checkpoint is different&#34;
                &#34; from the one obtained from compression_model_checkpoint...&#34;
                &#34;We do not support altering the compression model inside the LM &#34;
                &#34;checkpoint as parts of the code, in particular for running eval post-training &#34;
                &#34;will use the compression_model_checkpoint as the source of truth.&#34;)

    super().load_state_dict(state)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="audiocraft.solvers.musicgen.MusicGenSolver.run_generate_step"><code class="name flex">
<span>def <span class="ident">run_generate_step</span></span>(<span>self,<br>batch: Tuple[torch.Tensor, List[<a title="audiocraft.modules.conditioners.SegmentWithAttributes" href="../modules/conditioners.html#audiocraft.modules.conditioners.SegmentWithAttributes">SegmentWithAttributes</a>]],<br>gen_duration: float,<br>prompt_duration: float | None = None,<br>remove_text_conditioning: bool = False) ‑> dict</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@torch.no_grad()
def run_generate_step(self, batch: tp.Tuple[torch.Tensor, tp.List[SegmentWithAttributes]],
                      gen_duration: float, prompt_duration: tp.Optional[float] = None,
                      remove_text_conditioning: bool = False,
                      ) -&gt; dict:
    &#34;&#34;&#34;Run generate step on a batch of optional audio tensor and corresponding attributes.

    Args:
        batch (tuple[torch.Tensor, list[SegmentWithAttributes]]):
        use_prompt (bool): Whether to do audio continuation generation with prompt from audio batch.
        gen_duration (float): Target audio duration for the generation.
        prompt_duration (float, optional): Duration for the audio prompt to use for continuation.
    Returns:
        gen_outputs (dict): Generation outputs, consisting in audio, audio tokens from both the generation
            and the prompt along with additional information.
    &#34;&#34;&#34;
    bench_start = time.time()
    audio, meta = batch
    assert audio.size(0) == len(meta), (
        f&#34;Mismatch between number of items in audio batch ({audio.size(0)})&#34;,
        f&#34; and in metadata ({len(meta)})&#34;
    )
    # prepare attributes
    attributes = [x.to_condition_attributes() for x in meta]
    if remove_text_conditioning:
        attributes = _drop_description_condition(attributes)

    # prepare audio prompt
    if prompt_duration is None:
        prompt_audio = None
    else:
        assert prompt_duration &lt; gen_duration, &#34;Prompt duration must be lower than target generation duration&#34;
        prompt_audio_frames = int(prompt_duration * self.compression_model.sample_rate)
        prompt_audio = audio[..., :prompt_audio_frames]

    # get audio tokens from compression model
    if prompt_audio is None or prompt_audio.nelement() == 0:
        num_samples = len(attributes)
        prompt_tokens = None
    else:
        num_samples = None
        prompt_audio = prompt_audio.to(self.device)
        prompt_tokens, scale = self.compression_model.encode(prompt_audio)
        assert scale is None, &#34;Compression model in MusicGen should not require rescaling.&#34;

    # generate by sampling from the LM
    with self.autocast:
        total_gen_len = math.ceil(gen_duration * self.compression_model.frame_rate)
        gen_tokens = self.model.generate(
            prompt_tokens, attributes, max_gen_len=total_gen_len,
            num_samples=num_samples, **self.generation_params)

    # generate audio from tokens
    assert gen_tokens.dim() == 3
    gen_audio = self.compression_model.decode(gen_tokens, None)

    bench_end = time.time()
    gen_outputs = {
        &#39;rtf&#39;: (bench_end - bench_start) / gen_duration,
        &#39;ref_audio&#39;: audio,
        &#39;gen_audio&#39;: gen_audio,
        &#39;gen_tokens&#39;: gen_tokens,
        &#39;prompt_audio&#39;: prompt_audio,
        &#39;prompt_tokens&#39;: prompt_tokens,
    }
    return gen_outputs</code></pre>
</details>
<div class="desc"><p>Run generate step on a batch of optional audio tensor and corresponding attributes.</p>
<h2 id="args">Args</h2>
<dl>
<dt>batch (tuple[torch.Tensor, list[SegmentWithAttributes]]):</dt>
<dt><strong><code>use_prompt</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to do audio continuation generation with prompt from audio batch.</dd>
<dt><strong><code>gen_duration</code></strong> :&ensp;<code>float</code></dt>
<dd>Target audio duration for the generation.</dd>
<dt><strong><code>prompt_duration</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Duration for the audio prompt to use for continuation.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>gen_outputs (dict): Generation outputs, consisting in audio, audio tokens from both the generation
and the prompt along with additional information.</p></div>
</dd>
<dt id="audiocraft.solvers.musicgen.MusicGenSolver.show"><code class="name flex">
<span>def <span class="ident">show</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def show(self) -&gt; None:
    &#34;&#34;&#34;Show the compression model and LM model.&#34;&#34;&#34;
    self.logger.info(&#34;Compression model:&#34;)
    self.log_model_summary(self.compression_model)
    self.logger.info(&#34;LM model:&#34;)
    self.log_model_summary(self.model)</code></pre>
</details>
<div class="desc"><p>Show the compression model and LM model.</p></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="audiocraft.solvers.base.StandardSolver" href="base.html#audiocraft.solvers.base.StandardSolver">StandardSolver</a></b></code>:
<ul class="hlist">
<li><code><a title="audiocraft.solvers.base.StandardSolver.autocast" href="base.html#audiocraft.solvers.base.StandardSolver.autocast">autocast</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.best_metric_name" href="base.html#audiocraft.solvers.base.StandardSolver.best_metric_name">best_metric_name</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.commit" href="base.html#audiocraft.solvers.base.StandardSolver.commit">commit</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.common_train_valid" href="base.html#audiocraft.solvers.base.StandardSolver.common_train_valid">common_train_valid</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.evaluate" href="base.html#audiocraft.solvers.base.StandardSolver.evaluate">evaluate</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.generate" href="base.html#audiocraft.solvers.base.StandardSolver.generate">generate</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.initialize_ema" href="base.html#audiocraft.solvers.base.StandardSolver.initialize_ema">initialize_ema</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.load_checkpoints" href="base.html#audiocraft.solvers.base.StandardSolver.load_checkpoints">load_checkpoints</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.log_model_summary" href="base.html#audiocraft.solvers.base.StandardSolver.log_model_summary">log_model_summary</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.register_best_state" href="base.html#audiocraft.solvers.base.StandardSolver.register_best_state">register_best_state</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.register_ema" href="base.html#audiocraft.solvers.base.StandardSolver.register_ema">register_ema</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.restore" href="base.html#audiocraft.solvers.base.StandardSolver.restore">restore</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.run" href="base.html#audiocraft.solvers.base.StandardSolver.run">run</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.run_epoch" href="base.html#audiocraft.solvers.base.StandardSolver.run_epoch">run_epoch</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.run_one_stage" href="base.html#audiocraft.solvers.base.StandardSolver.run_one_stage">run_one_stage</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.run_step" href="base.html#audiocraft.solvers.base.StandardSolver.run_step">run_step</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.save_checkpoints" href="base.html#audiocraft.solvers.base.StandardSolver.save_checkpoints">save_checkpoints</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.should_run_stage" href="base.html#audiocraft.solvers.base.StandardSolver.should_run_stage">should_run_stage</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.should_stop_training" href="base.html#audiocraft.solvers.base.StandardSolver.should_stop_training">should_stop_training</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.train" href="base.html#audiocraft.solvers.base.StandardSolver.train">train</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.update_best_state_from_stage" href="base.html#audiocraft.solvers.base.StandardSolver.update_best_state_from_stage">update_best_state_from_stage</a></code></li>
<li><code><a title="audiocraft.solvers.base.StandardSolver.valid" href="base.html#audiocraft.solvers.base.StandardSolver.valid">valid</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="audiocraft.solvers" href="index.html">audiocraft.solvers</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="audiocraft.solvers.musicgen.MusicGenSolver" href="#audiocraft.solvers.musicgen.MusicGenSolver">MusicGenSolver</a></code></h4>
<ul class="">
<li><code><a title="audiocraft.solvers.musicgen.MusicGenSolver.DATASET_TYPE" href="#audiocraft.solvers.musicgen.MusicGenSolver.DATASET_TYPE">DATASET_TYPE</a></code></li>
<li><code><a title="audiocraft.solvers.musicgen.MusicGenSolver.build_dataloaders" href="#audiocraft.solvers.musicgen.MusicGenSolver.build_dataloaders">build_dataloaders</a></code></li>
<li><code><a title="audiocraft.solvers.musicgen.MusicGenSolver.build_model" href="#audiocraft.solvers.musicgen.MusicGenSolver.build_model">build_model</a></code></li>
<li><code><a title="audiocraft.solvers.musicgen.MusicGenSolver.evaluate_audio_generation" href="#audiocraft.solvers.musicgen.MusicGenSolver.evaluate_audio_generation">evaluate_audio_generation</a></code></li>
<li><code><a title="audiocraft.solvers.musicgen.MusicGenSolver.generate_audio" href="#audiocraft.solvers.musicgen.MusicGenSolver.generate_audio">generate_audio</a></code></li>
<li><code><a title="audiocraft.solvers.musicgen.MusicGenSolver.get_eval_solver_from_sig" href="#audiocraft.solvers.musicgen.MusicGenSolver.get_eval_solver_from_sig">get_eval_solver_from_sig</a></code></li>
<li><code><a title="audiocraft.solvers.musicgen.MusicGenSolver.get_formatter" href="#audiocraft.solvers.musicgen.MusicGenSolver.get_formatter">get_formatter</a></code></li>
<li><code><a title="audiocraft.solvers.musicgen.MusicGenSolver.initialize_optimization" href="#audiocraft.solvers.musicgen.MusicGenSolver.initialize_optimization">initialize_optimization</a></code></li>
<li><code><a title="audiocraft.solvers.musicgen.MusicGenSolver.load_from_pretrained" href="#audiocraft.solvers.musicgen.MusicGenSolver.load_from_pretrained">load_from_pretrained</a></code></li>
<li><code><a title="audiocraft.solvers.musicgen.MusicGenSolver.load_state_dict" href="#audiocraft.solvers.musicgen.MusicGenSolver.load_state_dict">load_state_dict</a></code></li>
<li><code><a title="audiocraft.solvers.musicgen.MusicGenSolver.run_generate_step" href="#audiocraft.solvers.musicgen.MusicGenSolver.run_generate_step">run_generate_step</a></code></li>
<li><code><a title="audiocraft.solvers.musicgen.MusicGenSolver.show" href="#audiocraft.solvers.musicgen.MusicGenSolver.show">show</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.5</a>.</p>
</footer>
</body>
</html>
